<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="MFIN7036 Students 2025" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Group AstroNLP, Reflective Report, " />

<meta property="og:title" content="AstroNLP 01: Web Scraping Financial News Sites with Python: Challenges and Solutions "/>
<meta property="og:url" content="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/astronlp-01-web-scraping-financial-news-sites-with-python-challenges-and-solutions.html" />
<meta property="og:description" content="By Group &#34;AstroNLP&#34; The analysis shown in the blog is strictly from a financial and market impact perspective. AstroNLP 01: Web Scraping Financial News Sites with Python: Challenges and Solutions Introduction: The Allure and Challenges of Financial Data Scraping In today&#39;s data-driven financial world, accessing real-time news from premium sources …" />
<meta property="og:site_name" content="MFIN7036 Student Blog 2025" />
<meta property="og:article:author" content="MFIN7036 Students 2025" />
<meta property="og:article:published_time" content="2026-01-10T13:14:00+08:00" />
<meta name="twitter:title" content="AstroNLP 01: Web Scraping Financial News Sites with Python: Challenges and Solutions ">
<meta name="twitter:description" content="By Group &#34;AstroNLP&#34; The analysis shown in the blog is strictly from a financial and market impact perspective. AstroNLP 01: Web Scraping Financial News Sites with Python: Challenges and Solutions Introduction: The Allure and Challenges of Financial Data Scraping In today&#39;s data-driven financial world, accessing real-time news from premium sources …">

        <title>AstroNLP 01: Web Scraping Financial News Sites with Python: Challenges and Solutions  · MFIN7036 Student Blog 2025
</title>
        <link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="MFIN7036 Student Blog 2025 - Full Atom Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/"><span class=site-name>MFIN7036 Student Blog 2025</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://buehlmaier.github.io/MFIN7036-student-blog-2025-12
                                    >Home</a>
                                </li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html">Categories</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html">Tags</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/astronlp-01-web-scraping-financial-news-sites-with-python-challenges-and-solutions.html">
                AstroNLP 01:  Web Scraping Financial News Sites with Python: Challenges and Solutions
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>By Group "AstroNLP"</p>
<blockquote>
<blockquote>
<p><em>The analysis shown in the blog is strictly from a financial and market impact perspective.</em></p>
</blockquote>
</blockquote>
<h1>AstroNLP 01:  Web Scraping Financial News Sites with Python: Challenges and Solutions</h1>
<h2>Introduction: The Allure and Challenges of Financial Data Scraping</h2>
<p>In today's data-driven financial world, accessing real-time news from premium sources like Bloomberg, Reuters, and The Wall Street Journal can provide valuable insights for investors and analysts. As Python developers, we recently embarked on a project to build a news aggregator focusing on gold price movements, targeting these three major financial news platforms. What seemed straightforward initially turned into a fascinating journey through the complex landscape of modern web scraping challenges.</p>
<h2>The Initial Approach: Naive Scraping</h2>
<p>Our initial code structure was simple - using <code>requests</code> to fetch pages and <code>BeautifulSoup</code> to parse them. I created a <code>NewsCrawler</code> class with methods for each news source, expecting to extract article titles, dates, content, and URLs. The basic structure looked like this:</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">NewsCrawler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;en-US,en;q=0.9&#39;</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">search_bloomberg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keyword</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>

        <span class="c1"># Basic implementation</span>

        <span class="k">pass</span>
</code></pre></div>

<h2>Challenge 1: Anti-Scraping Measures</h2>
<h3>The Problem: Getting Blocked</h3>
<p>Within minutes of running the script, the code started encountering 403 errors and CAPTCHA pages. Bloomberg and Reuters both employ sophisticated anti-scraping measures that detect non-human browsing patterns. The Wall Street Journal was even more restrictive, immediately blocking requests that didn't come from authenticated sessions.</p>
<h3>Solution 1: Enhanced Headers and Session Management</h3>
<p>We learned that simple User-Agent strings weren't enough. News sites check for multiple headers and session consistency. Here's our enhanced approach:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;en-US,en;q=0.5&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Accept-Encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip, deflate, br&#39;</span><span class="p">,</span>
        <span class="s1">&#39;DNT&#39;</span><span class="p">:</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Connection&#39;</span><span class="p">:</span> <span class="s1">&#39;keep-alive&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Upgrade-Insecure-Requests&#39;</span><span class="p">:</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Sec-Fetch-Dest&#39;</span><span class="p">:</span> <span class="s1">&#39;document&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Sec-Fetch-Mode&#39;</span><span class="p">:</span> <span class="s1">&#39;navigate&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Sec-Fetch-Site&#39;</span><span class="p">:</span> <span class="s1">&#39;none&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Sec-Fetch-User&#39;</span><span class="p">:</span> <span class="s1">&#39;?1&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Cache-Control&#39;</span><span class="p">:</span> <span class="s1">&#39;max-age=0&#39;</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="p">)</span>
</code></pre></div>

<h3>Solution 2: Proxy Rotation</h3>
<p>To avoid IP-based blocking, we implemented proxy rotation. While I can't share actual proxy lists, here's the structure:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_get_request_with_proxy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;http&#39;</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proxy_list</span><span class="p">),</span>
        <span class="s1">&#39;https&#39;</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proxy_list</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="c1"># Fallback to direct connection</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>

<h2>Challenge 2: Rate Limiting and Behavioral Detection</h2>
<h3>The Problem: Too Fast, Too Predictable</h3>
<p>Even with proper headers, the requests were getting blocked because they followed predictable patterns with consistent timing between requests.</p>
<h3>Solution: Randomized Delays and Human-Like Behavior</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_human_like_delay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Random delays between requests to mimic human reading patterns&quot;&quot;&quot;</span>
    <span class="c1"># Vary delay based on action type</span>
    <span class="n">delay_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;short&#39;</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>      <span class="c1"># Between pages</span>
        <span class="p">(</span><span class="s1">&#39;medium&#39;</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">)),</span>     <span class="c1"># Reading article list</span>
        <span class="p">(</span><span class="s1">&#39;long&#39;</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span>      <span class="c1"># Reading full article</span>
    <span class="p">]</span>
    <span class="n">delay_type</span><span class="p">,</span> <span class="n">delay</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">delay_types</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">delay</span><span class="p">)</span>

    <span class="c1"># Occasionally add extra random pauses</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.2</span><span class="p">:</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">delay_type</span>
</code></pre></div>

<p>Additionally, we simulated scrolling behavior to better mimic human interaction:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_scrolling_simulation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simulate scrolling behavior&quot;&quot;&quot;</span>
    <span class="n">scroll_positions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1200</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="n">scroll_positions</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.7</span><span class="p">:</span>  <span class="c1"># 70% chance to pause at each scroll point</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
</code></pre></div>

<h2>Challenge 3: Paywalls and Subscription Content</h2>
<h3>The Problem: Incomplete Article Access</h3>
<p>The Wall Street Journal presented the biggest challenge - most content is behind a paywall. Even Bloomberg and Reuters limit article views for non-subscribers. Traditional scraping approaches fail when confronted with subscription requirements that hide content behind login screens or partial previews.</p>
<h3>Solution: Multi-Source Verification and Abstract Collection</h3>
<p>Since bypassing paywalls ethically isn't possible, we adjusted our strategy to focus on publicly accessible content:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">search_wsj</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keyword</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Handle WSJ&#39;s subscription requirements&quot;&quot;&quot;</span>
    <span class="n">articles</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Search for publicly accessible content</span>
    <span class="n">search_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;https://www.wsj.com/search/term.html?KEYWORDS=</span><span class="si">{</span><span class="n">keyword</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div>

<p>The key insight was to search for articles with 'free' or 'teaser' classes, which often contain partial content accessible without subscription:</p>
<div class="codehilite"><pre><span></span><code><span class="k">try</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">search_url</span><span class="p">)</span>
    <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="s1">&#39;html.parser&#39;</span><span class="p">)</span>

    <span class="c1"># Look for articles with &#39;free&#39; or &#39;teaser&#39; classes</span>
    <span class="n">free_articles</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;.*free.*|.*teaser.*&#39;</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">article</span> <span class="ow">in</span> <span class="n">free_articles</span><span class="p">:</span>
        <span class="c1"># Extract available preview content</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">article</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;h2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span> <span class="k">if</span> <span class="n">article</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;h2&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="n">excerpt</span> <span class="o">=</span> <span class="n">article</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span> <span class="k">if</span> <span class="n">article</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>

        <span class="k">if</span> <span class="n">title</span> <span class="ow">and</span> <span class="s1">&#39;gold&#39;</span> <span class="ow">in</span> <span class="n">title</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="n">articles</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">title</span><span class="p">,</span>
                <span class="s1">&#39;excerpt&#39;</span><span class="p">:</span> <span class="n">excerpt</span><span class="p">[:</span><span class="mi">500</span><span class="p">],</span>  <span class="c1"># Limited preview</span>
                <span class="s1">&#39;source&#39;</span><span class="p">:</span> <span class="s1">&#39;Wall Street Journal&#39;</span><span class="p">,</span>
                <span class="s1">&#39;requires_subscription&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="n">article</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)[</span><span class="s1">&#39;href&#39;</span><span class="p">]</span> <span class="k">if</span> <span class="n">article</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
            <span class="p">})</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WSJ access limited: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">return</span> <span class="n">articles</span>
</code></pre></div>

<p>This approach allowed us to collect article metadata and preview text, providing valuable information even when full content was unavailable. While not ideal, it represented a practical compromise between data completeness and ethical scraping practices.</p>
<h2>Challenge 4: Changing Website Structures</h2>
<h3>The Problem: Broken Selectors</h3>
<p>Financial websites frequently update their layouts and CSS classes, breaking carefully crafted selectors. A scraper that worked perfectly one day might fail completely the next as sites deploy new designs or change their HTML structure.</p>
<h3>Solution: Robust Selector Strategies and Monitoring</h3>
<p>To address this, we implemented a fallback system that tries multiple selector patterns:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_find_with_fallback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">soup</span><span class="p">,</span> <span class="n">selectors</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Try multiple selector patterns&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">selector</span> <span class="ow">in</span> <span class="n">selectors</span><span class="p">:</span>
        <span class="n">element</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">select_one</span><span class="p">(</span><span class="n">selector</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">element</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">element</span>

    <span class="c1"># If no selector works, use more generic approach</span>
    <span class="n">possible_elements</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">([</span><span class="s1">&#39;h1&#39;</span><span class="p">,</span> <span class="s1">&#39;h2&#39;</span><span class="p">,</span> <span class="s1">&#39;h3&#39;</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">possible_elements</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">keyword</span> <span class="ow">in</span> <span class="n">elem</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;gold&#39;</span><span class="p">,</span> <span class="s1">&#39;precious&#39;</span><span class="p">,</span> <span class="s1">&#39;metal&#39;</span><span class="p">]):</span>
            <span class="k">return</span> <span class="n">elem</span>

    <span class="k">return</span> <span class="kc">None</span>
</code></pre></div>

<p>For extracting publication dates, we created a flexible approach that searches through multiple potential date locations:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_extract_date_flexible</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">soup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiple strategies to find publication date&quot;&quot;&quot;</span>
    <span class="n">date_patterns</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;meta&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;property&#39;</span><span class="p">:</span> <span class="s1">&#39;article:published_time&#39;</span><span class="p">}),</span>
        <span class="p">(</span><span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="p">{}),</span>
        <span class="p">(</span><span class="s1">&#39;span&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;.*date.*|.*time.*&#39;</span><span class="p">)}),</span>
        <span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;.*timestamp.*&#39;</span><span class="p">)})</span>
    <span class="p">]</span>

<span class="k">for</span> <span class="n">tag_name</span><span class="p">,</span> <span class="n">attrs</span> <span class="ow">in</span> <span class="n">date_patterns</span><span class="p">:</span>
    <span class="n">element</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">tag_name</span><span class="p">,</span> <span class="n">attrs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">element</span><span class="p">:</span>
        <span class="n">date_text</span> <span class="o">=</span> <span class="n">element</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;datetime&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">element</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">element</span><span class="o">.</span><span class="n">text</span>
        <span class="k">if</span> <span class="n">date_text</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_date</span><span class="p">(</span><span class="n">date_text</span><span class="p">)</span>

<span class="k">return</span> <span class="kc">None</span>
</code></pre></div>

<p>This multi-strategy approach significantly improved our scraper's resilience. By trying multiple common patterns for locating critical information, we reduced the frequency of complete failures when websites changed their markup.</p>
<h2>Conclusion: Lessons Learned</h2>
<p>Building a financial news scraper taught us that modern web scraping is less about parsing HTML and more about understanding and mimicking human behavior. The technical challenges were significant, but each obstacle provided an opportunity to learn more about how websites protect their content and how to responsibly gather public information.</p>
<p>The key takeaways were:</p>
<ul>
<li>Anti-scraping measures are sophisticated and constantly evolving</li>
<li>Multiple strategies (headers, delays, proxies) work better than any single approach</li>
<li>Sometimes, accepting limitations (like paywalls) is necessary</li>
<li>Robust code handles failures gracefully and continues operation</li>
</ul>
<p>Throughout this project, we adhered to several ethical guidelines:</p>
<ol>
<li><strong>Respect robots.txt</strong>: Always check and comply with each site's robots.txt file</li>
<li><strong>Limit request frequency</strong>: Never overload servers with too many requests</li>
<li><strong>Use publicly available data</strong>: Focus on content that doesn't require authentication</li>
<li><strong>Attribute properly</strong>: Always credit sources when using their content</li>
<li><strong>Consider APIs first</strong>: Where available, use official APIs instead of scraping</li>
</ol>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2026-01-10T13:14:00+08:00">Sat 10 January 2026</time>
            <h4>Category</h4>
            <a class="category-link" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html#reflective-report-ref">Reflective Report</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html#group-astronlp-ref">Group AstroNLP
                    <span class="superscript">3</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/buehlmaier/MFIN7036-student-blog-2025-12" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>