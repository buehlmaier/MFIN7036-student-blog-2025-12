<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="MFIN7036 Students 2025" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Group LexiCore, Reflective Report, " />

<meta property="og:title" content="Capture Co-occurrence Frequency: A Way To Read Market Sentiment (by Group &#34;LexiCore&#34;) "/>
<meta property="og:url" content="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/capture-co-occurrence-frequency-a-way-to-read-market-sentiment-by-group-lexicore.html" />
<meta property="og:description" content="By Group &#34;LexiCore&#34; To Begin With After the first pre-phase, we carried out iterations and replacements for the data sources and data extraction methods in the project. Then, our research goes in depth. This blog shows our approach to read market sentiment and its effect by catching co-occurrence frequency of …" />
<meta property="og:site_name" content="MFIN7036 Student Blog 2025" />
<meta property="og:article:author" content="MFIN7036 Students 2025" />
<meta property="og:article:published_time" content="2026-01-10T19:30:00+08:00" />
<meta name="twitter:title" content="Capture Co-occurrence Frequency: A Way To Read Market Sentiment (by Group &#34;LexiCore&#34;) ">
<meta name="twitter:description" content="By Group &#34;LexiCore&#34; To Begin With After the first pre-phase, we carried out iterations and replacements for the data sources and data extraction methods in the project. Then, our research goes in depth. This blog shows our approach to read market sentiment and its effect by catching co-occurrence frequency of …">

        <title>Capture Co-occurrence Frequency: A Way To Read Market Sentiment (by Group &#34;LexiCore&#34;)  · MFIN7036 Student Blog 2025
</title>
        <link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="MFIN7036 Student Blog 2025 - Full Atom Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/"><span class=site-name>MFIN7036 Student Blog 2025</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://buehlmaier.github.io/MFIN7036-student-blog-2025-12
                                    >Home</a>
                                </li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html">Categories</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html">Tags</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/capture-co-occurrence-frequency-a-way-to-read-market-sentiment-by-group-lexicore.html">
                Capture Co-occurrence Frequency: A Way To Read Market Sentiment (by Group "LexiCore")
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>By Group "LexiCore"</p>
<h1>To Begin With</h1>
<p>After the first pre-phase, we carried out iterations and replacements for the data
 sources and data extraction methods in the project. Then, our research goes in depth.
  This blog shows our approach to read market sentiment and its effect by catching 
  co-occurrence frequency of specific words. It mainly focuses on our ideas, problems 
  we met and our attempts to solve them. If you also encounter similar problems and can 
  identify some solutions or methods, please feel free to contact us. Thank you very much!</p>
<h1>From Capital IQ To Python: Automatically Construct A Database Of Top 50 US Tech Stocks</h1>
<p>In order to obtain better stock data for analysis, we combined Capital IQ with yfinance, wrote an automatic data scraping program and automatically completed the preprocessing.</p>
<h2>Data Source Filtering</h2>
<p>How to define 'TOP 50'? We use the most conspicuous standard: market value. We construct a <strong>dictionary</strong> through the Screening tool in Capital IQ.</p>
<p><img alt="CapitalIQ-Screening" src="\images\LexiCore_01_Capital-IQ-01.png"></p>
<p>If the stock is listed in not only US market, pay attention that you should only keep its ticker in US. (TSM but not 2330 stands for Taiwan Semiconductor Manufacturing Company, etc.)</p>
<p><img alt="CapitalIQ-Result" src="\images\LexiCore_02_Capital-IQ-02.png"></p>
<h2>Automatical Construction: yfinance</h2>
<p>In the latest API update, yfinance may fail to find the "Adj-Close" column due to automatic price adjustment. 
Therefore, a dynamic detection logic has been added to the code to ensure that the adjusted closing price, which takes into account stock splits and dividends, is obtained.</p>
<div class="highlight"><pre><span></span><code><span class="n">start_date</span> <span class="o">=</span> <span class="s2">&quot;2020-01-01&quot;</span>
<span class="n">end_date</span> <span class="o">=</span> <span class="s2">&quot;2025-12-31&quot;</span>
<span class="c1"># ban auto_adjust</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">tickers</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">start_date</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">end_date</span><span class="p">,</span> <span class="n">auto_adjust</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">threads</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#Prioritize extracting AdjClose (revised price)</span>
<span class="n">available_columns</span> <span class="o">=</span> <span class="n">raw_data</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="k">if</span> <span class="s1">&#39;Adj Close&#39;</span> <span class="ow">in</span> <span class="n">available_columns</span><span class="p">:</span>
    <span class="n">prices</span> <span class="o">=</span> <span class="n">raw_data</span><span class="p">[</span><span class="s1">&#39;Adj Close&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;succeed in extracting&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">prices</span> <span class="o">=</span> <span class="n">raw_data</span><span class="p">[</span><span class="s1">&#39;Close&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;warning: replacing with &#39;close&#39;&quot;</span><span class="p">)</span>   
<span class="c1">#Calculate the daily simple return rate (Returns)</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">prices</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">prices</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;tech top50 prices.csv&quot;</span><span class="p">)</span>
<span class="n">returns</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;tech top50 returns.csv&quot;</span><span class="p">)</span>
</code></pre></div>

<h1>Collecting Social Media Data From Kaggle</h1>
<p>After failing to get historical data from Reddit and X (Twitter) directly through APIs, we turn to some existing databases. Here, we found a well established dataset posted on Kaggle.</p>
<p><a href="https://www.kaggle.com/datasets/bwandowando/reddit-rnews-subreddit-2008-to-2024/data?select=news_comments_018.parquet">News&amp;Comments from 2008 to 2024 on Reddit</a></p>
<p>Part of our code is shown below. </p>
<div class="highlight"><pre><span></span><code><span class="n">s</span> <span class="o">=</span> <span class="n">df_2020_en</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="c1"># regex for keywords</span>
<span class="n">topic_patterns</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\b(?:</span><span class="si">%s</span><span class="s2">)\b&quot;</span> <span class="o">%</span> <span class="s2">&quot;|&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">,</span> <span class="n">v</span><span class="p">)),</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">IGNORECASE</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">topic_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">}</span>
<span class="c1"># regex for companies</span>
<span class="n">company_patterns</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(?:</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="s2">&quot;|&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">,</span> <span class="n">v</span><span class="p">)),</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">IGNORECASE</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">company_alias</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">}</span>
</code></pre></div>

<p>It should be noted that some filtering and merging operations should be added to the code to make the database usable. Here is what we have done.</p>
<div class="highlight"><pre><span></span><code><span class="c1">#Convert the time variables into the same format</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;created_dt&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;created&quot;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;coerce&quot;</span><span class="p">)</span>
<span class="c1">#Filter out titles that contain keywords (i.e. match the regex patterns defined)</span>
<span class="n">topic_hits</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">na</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">pat</span> <span class="ow">in</span> <span class="n">topic_patterns</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
<span class="n">company_hits</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">na</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">pat</span> <span class="ow">in</span> <span class="n">company_patterns</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
<span class="c1"># Need to fit at least one topic pattern or one company pattern</span>
<span class="n">mask_any</span> <span class="o">=</span> <span class="n">topic_hits</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">company_hits</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df_2020_en</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">mask_any</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div>

<h1>Frequency Count</h1>
<p>As we wrote in our first blog, we downloaded the title of related news from Guardian and New York Times. We believe that the message within the news can be well reflected by performing NLP on their titles.</p>
<p>By counting the frequency of specific words written in these titles, we can estimate the media attention on technology companies or news every day. Due to <strong>different writing styles</strong> of news media, we need to 
adjust the approach in our code from time to time. Here, we only post an example, so if you are trying to use them in your own project (which is totally welcomed), make sure to its feasibility based on your data sources.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">calculate_daily_frequency</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">topic_patterns</span><span class="o">=</span><span class="n">topic_patterns</span><span class="p">,</span> <span class="n">company_patterns</span><span class="o">=</span><span class="n">company_patterns</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>  
    <span class="c1"># Match topic patterns</span>
    <span class="n">topic_hits</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">na</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">pat</span> <span class="ow">in</span> <span class="n">topic_patterns</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">})</span>    
    <span class="c1"># Match company patterns</span>
    <span class="n">company_hits</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">na</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">pat</span> <span class="ow">in</span> <span class="n">company_patterns</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">})</span>   
    <span class="c1"># Match topic pattern and company pattern</span>
    <span class="n">mask_any</span> <span class="o">=</span> <span class="n">topic_hits</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">company_hits</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
    <span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">mask_any</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># Calculate total news count per day</span>
    <span class="n">daily_total</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">df</span>
        <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;total_count&quot;</span><span class="p">)</span>
    <span class="p">)</span>    
    <span class="c1"># Calculate matched news count per day</span>
    <span class="n">daily_filtered</span> <span class="o">=</span> <span class="n">df_filtered</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span>
        <span class="n">count</span><span class="o">=</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">),</span>  <span class="c1"># Count news per day</span>
        <span class="n">mean_combined_score</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;combined_score&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> 
    <span class="p">)</span>   
    <span class="c1"># Merge results</span>
    <span class="n">daily_counts</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">daily_total</span>
        <span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>
        <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">daily_filtered</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="p">)</span>  
    <span class="c1"># Convert types and calculate frequency</span>
    <span class="n">daily_counts</span><span class="p">[</span><span class="s2">&quot;filtered_count&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">daily_counts</span><span class="p">[</span><span class="s2">&quot;count&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">daily_counts</span><span class="p">[</span><span class="s2">&quot;frequency&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">daily_counts</span><span class="p">[</span><span class="s2">&quot;filtered_count&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">daily_counts</span><span class="p">[</span><span class="s2">&quot;total_count&quot;</span><span class="p">]</span>
    <span class="p">)</span>    
    <span class="k">return</span> <span class="n">daily_counts</span><span class="p">[[</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;total_count&#39;</span><span class="p">,</span> <span class="s1">&#39;filtered_count&#39;</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_combined_score&#39;</span><span class="p">]]</span>
</code></pre></div>

<h1>Simple Sentiment Analysis</h1>
<p>We combined different kinds of sentiment analysis approaches to better estimate the market sentiment. It includes:</p>
<ul>
<li><strong>VADER</strong>: Optimized for social media/text with emojis and slang</li>
<li><strong>TextBlob</strong>: Rule-based sentiment analysis with polarity and subjectivity</li>
<li><strong>Combined Approach</strong>: Uses both methods for robust classification</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">analyze_sentiment_vader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>     
    <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vader_analyzer</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>   
    <span class="c1"># Determine sentiment label based on compound score</span>
    <span class="n">compound</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;compound&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">compound</span> <span class="o">&gt;=</span> <span class="mf">0.05</span><span class="p">:</span>
        <span class="n">sentiment_label</span> <span class="o">=</span> <span class="s1">&#39;positive&#39;</span>
    <span class="k">elif</span> <span class="n">compound</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">:</span>
        <span class="n">sentiment_label</span> <span class="o">=</span> <span class="s1">&#39;negative&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sentiment_label</span> <span class="o">=</span> <span class="s1">&#39;neutral&#39;</span>         
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;vader_compound&#39;</span><span class="p">:</span> <span class="n">compound</span><span class="p">,</span>
        <span class="s1">&#39;vader_positive&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">],</span>
        <span class="s1">&#39;vader_negative&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">],</span>
        <span class="s1">&#39;vader_neutral&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;neu&#39;</span><span class="p">],</span>
        <span class="s1">&#39;vader_sentiment&#39;</span><span class="p">:</span> <span class="n">sentiment_label</span>
    <span class="p">}</span>    
<span class="k">def</span> <span class="nf">analyze_sentiment_textblob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span> <span class="ow">or</span> <span class="n">pd</span><span class="o">.</span><span class="n">isna</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;textblob_polarity&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;textblob_subjectivity&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;textblob_sentiment&#39;</span><span class="p">:</span> <span class="s1">&#39;neutral&#39;</span>
        <span class="p">}</span>

    <span class="n">blob</span> <span class="o">=</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="n">polarity</span> <span class="o">=</span> <span class="n">blob</span><span class="o">.</span><span class="n">sentiment</span><span class="o">.</span><span class="n">polarity</span>
    <span class="n">subjectivity</span> <span class="o">=</span> <span class="n">blob</span><span class="o">.</span><span class="n">sentiment</span><span class="o">.</span><span class="n">subjectivity</span>

    <span class="c1"># Determine sentiment label</span>
    <span class="k">if</span> <span class="n">polarity</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sentiment_label</span> <span class="o">=</span> <span class="s1">&#39;positive&#39;</span>
    <span class="k">elif</span> <span class="n">polarity</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sentiment_label</span> <span class="o">=</span> <span class="s1">&#39;negative&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sentiment_label</span> <span class="o">=</span> <span class="s1">&#39;neutral&#39;</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;textblob_polarity&#39;</span><span class="p">:</span> <span class="n">polarity</span><span class="p">,</span>
        <span class="s1">&#39;textblob_subjectivity&#39;</span><span class="p">:</span> <span class="n">subjectivity</span><span class="p">,</span>
        <span class="s1">&#39;textblob_sentiment&#39;</span><span class="p">:</span> <span class="n">sentiment_label</span>
    <span class="p">}</span>
</code></pre></div>

<p>Using these approaches, we can simply get a tentative view. We seperated several factors for further analysis:</p>
<ul>
<li><strong>Compound Score</strong> (-1 to 1): Overall sentiment intensity</li>
<li><strong>Polarity</strong> (-1 to 1): Sentiment direction</li>
<li><strong>Subjectivity</strong> (0 to 1): How opinionated vs factual</li>
<li><strong>Confidence Score</strong>: Agreement between methods</li>
<li><strong>Categorical Labels</strong>: 'positive', 'negative', 'neutral'</li>
</ul>
<h1>TF-IDF Calculation</h1>
<p>TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical method used to evaluate the <strong>importance</strong> of a word in a set of documents. It measures the value of a word in distinguishing documents by multiplying the frequency of the word in a document (TF) with the rarity of the word in the entire corpus (IDF).</p>
<p>We choose this method to better estimate in news, because frequency can only tell part of the story. If a word has a high TF-IDF value, who appears frequently in one article but rarely in other articles, then this word is the "key word" of that particular document. In our project, a higher TF-IDF value indicates that the emotions expressed in this text should be given more weight.</p>
<div class="highlight"><pre><span></span><code><span class="k">if</span> <span class="n">use_custom_vocab</span> <span class="ow">and</span> <span class="n">topic_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
     <span class="c1"># create a set of keywords</span>
     <span class="n">custom_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
     <span class="k">if</span> <span class="n">topic_dict</span><span class="p">:</span>
         <span class="k">for</span> <span class="n">category</span><span class="p">,</span> <span class="n">keywords</span> <span class="ow">in</span> <span class="n">topic_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
             <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">:</span>
                 <span class="c1"># Only keep the keywords that is not long</span>
                 <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">keyword</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span> 
                     <span class="n">custom_vocab</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keyword</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>    
     <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">custom_vocab</span><span class="p">)[:</span><span class="n">max_features</span><span class="p">]</span>
     <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using custom vocabulary with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span><span class="si">}</span><span class="s2"> terms&quot;</span><span class="p">)</span>    
     <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span>
         <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span>
         <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span>
         <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>  <span class="c1"># allow at most 2-gram</span>
         <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>  <span class="c1"># ignore the words that appear in 95% of the corpus</span>
         <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  
         <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span>
     <span class="p">)</span>
 <span class="k">else</span><span class="p">:</span>
     <span class="c1"># Atomatically extract keywords if no dictionary is provided</span>
     <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span>
         <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
         <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span>
         <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
         <span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
         <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
         <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span>
     <span class="p">)</span> 
 <span class="c1"># calculate TF-IDF matrix</span>
 <span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div>

<h1>OLS Regression &amp; Machine Learning</h1>
<p>We employed various modeling methods to attempt to link the emotional factors we extracted with the excess returns of technology stocks.</p>
<p>Due to the limited time, we directly downloaded the β value for each stock from Capital IQ, 
rather than making predictions based on historical data. It might lead to higher 
estimation error in excess return.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Prepare dependent and indepedent varaible</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;freq_g_c&#39;</span><span class="p">,</span> <span class="s1">&#39;sent_g_c&#39;</span><span class="p">,</span> <span class="s1">&#39;freq_g_t&#39;</span><span class="p">,</span> <span class="s1">&#39;sent_g_t&#39;</span><span class="p">,</span> <span class="s1">&#39;freq_n_t&#39;</span><span class="p">,</span> <span class="s1">&#39;sent_n_t&#39;</span><span class="p">,</span><span class="s1">&#39;freq_r&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;return&#39;</span><span class="p">]</span>
<span class="c1"># Add constant</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># Conduct regression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</code></pre></div>

<p>The initial results were not satisfactory. We carefully reviewed the data processing procedure mentioned earlier, corrected some extreme value issues, and attempted to incorporate a time window, which led to more significant regression results.
According to our attempts, set the time window to be <strong>4 days</strong> after the document is published will 
obtain a better estimation result.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Add a time window</span>
<span class="n">result_timewindow</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">result_timewindow</span><span class="p">[</span><span class="s1">&#39;excess return&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_timewindow</span><span class="p">[</span><span class="s1">&#39;excess return&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">result_timewindow</span> <span class="o">=</span> <span class="n">result_timewindow</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
</code></pre></div>

<p>A series of machine learning models were adopted too, and we try to enhance their performance by adjusting parameters. 
Here we use Random forest as an example. The R-square result is not as good as OLS.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Separate features and labels</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">result_timewindow</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;return&#39;</span><span class="p">,</span> <span class="s1">&#39;excess return&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># All TF-IDF features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">result_timewindow</span><span class="p">[</span><span class="s1">&#39;excess return&#39;</span><span class="p">]</span>
<span class="c1"># Split data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Random Forest Regression</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Simple parameter tuning</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># Use best model</span>
<span class="n">best_rf</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred_rf</span> <span class="o">=</span> <span class="n">best_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># Feature importance</span>
<span class="n">feature_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;feature&#39;</span><span class="p">:</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="n">best_rf</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="p">})</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p>Though our machine learning results are not so ideal, these factor importance may also be enlightening.</p>
<h1>Takeaway</h1>
<p>After several weeks of trial and adjustment, we initially established a set of Python methods for data acquisition, data cleaning, and extraction of sentiment factors. We then used the extracted factors to attempt to identify their correlations with the stock market. Although our results were not particularly outstanding, we still learned a great many key points:</p>
<ol>
<li>
<p><strong>Data means everything</strong>. Extracting media data, especially from social media platform 
could be much more difficult than you imagine. A possible solution is to look for the job done by previous researchers.</p>
</li>
<li>
<p><strong>There is always a way out even in the darkest times</strong>. Python is a vast treasure trove. The problem you are currently facing might be troubling someone in another corner of the world. Therefore, make good use of community resources such as GitHub.</p>
</li>
<li>
<p><strong>Sentiment is fleeting</strong>. Capturing market sentiment is often very challenging, and investment opportunities derived from historical data analysis may disappear before the code is fully executed.</p>
</li>
</ol>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2026-01-10T19:30:00+08:00">Sat 10 January 2026</time>
            <h4>Category</h4>
            <a class="category-link" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html#reflective-report-ref">Reflective Report</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html#group-lexicore-ref">Group LexiCore
                    <span class="superscript">2</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/buehlmaier/MFIN7036-student-blog-2025-12" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>