<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>MFIN7036 Student Blog 2025 - Blog</title><link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/" rel="alternate"></link><link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/feeds/blog.atom.xml" rel="self"></link><id>https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/</id><updated>2026-01-21T19:22:00+08:00</updated><entry><title>When Market is Swayed by Trump Sentiment, Arbitrage Opportunities Emerged (by Group "PoliVoli")</title><link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/when-market-is-swayed-by-trump-sentiment-arbitrage-opportunities-emerged-by-group-polivoli.html" rel="alternate"></link><published>2026-01-21T19:22:00+08:00</published><updated>2026-01-21T19:22:00+08:00</updated><author><name>MFIN7036 Students 2025</name></author><id>tag:buehlmaier.github.io,2026-01-21:/MFIN7036-student-blog-2025-12/when-market-is-swayed-by-trump-sentiment-arbitrage-opportunities-emerged-by-group-polivoli.html</id><summary type="html">&lt;p&gt;By Group "PoliVoli"&lt;/p&gt;
&lt;h2&gt;X Selection&lt;/h2&gt;
&lt;p&gt;Currently, there are two approaches within the team regarding the selection of the X variable. The first involves using the outputs from three distinct sentiment models as the variable. The second approach does not rely on other sentiment models; instead, it exclusively employs the tokenization …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "PoliVoli"&lt;/p&gt;
&lt;h2&gt;X Selection&lt;/h2&gt;
&lt;p&gt;Currently, there are two approaches within the team regarding the selection of the X variable. The first involves using the outputs from three distinct sentiment models as the variable. The second approach does not rely on other sentiment models; instead, it exclusively employs the tokenization tool provided by ProsusAI within Finbert to vectorize Trump's speech for independent training.&lt;/p&gt;
&lt;p&gt;Corresponding to each different X variable, we conducted separate model training sessions.&lt;/p&gt;
&lt;h2&gt;Y Selection&lt;/h2&gt;
&lt;p&gt;In the previous blog, we proposed three methods for constructing Y: labeling daily volatility using the population sample mean or median; assessing the persistent effect of X using delta+n; and determining the short-term effect by subtracting the day's low from its high. Given the interpretability of the framework and regression results, direct labeling via mean or median proved infeasible. However, the combined approach using methods two and three—estimated by ‘h-l day+1’ (the next day's high-low range)—demonstrated both economic and statistical significance. 'large vol' is calculated based on ‘h-l day+1’ to determine whether this value ranks within the top 25% (75th percentile) over the past 40 days. Using 20% would result in insufficient data meeting the subsequent overlay requirements. The 25% threshold represents one-quarter, aligning with standard practice. Verification confirmed all desired dates (e.g., April 7th tariffs) meet this criterion. The final calculated variable ‘y’ is ‘pos large’, signifying a positive large volatility event. Its underlying logic is: if (‘delta+1’ &amp;gt; 0), then ‘delta+1’; otherwise, 0. This means if the large volatility is a sharp decline in VIX, it would be 0. Only a significant upward volatility on the following day would yield a value of 1. ‘delta+1’ is calculated as the next day's closing price divided by today's closing price minus 1. The reason for distinguishing positive and negative values is our assumption that Trump's speeches will increase volatility. However, if we only judged amplitude (high-low), a significant drop in volatility would also yield a value of 1, contradicting our original intent. Therefore, we need to add a layer of directional judgment.&lt;/p&gt;
&lt;p&gt;The primary code as follows:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt; 
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timedelta&lt;/span&gt; 

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;(high-low)/today_price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;recent_40_days_75%_percentile&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rolling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_periods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;39&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;whether_big_vol&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;(high-low)/today_value&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;recent_40days_75%_percentile&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;(high-low)/today_value&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;(high-low)/today_value&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Random Forest Tree Model&lt;/h2&gt;
&lt;h4&gt;Feature Construction (X Selection)&lt;/h4&gt;
&lt;p&gt;Yang et al. (2020) show that FinBERT performs well in financial tasks such as sentiment analysis and event extraction, and that its embeddings preserve financial semantics, making them suitable for time-series financial data. So, in our project, we use the FinBERT model to train the text data to gain the daily superimposed vectors of Trump and use them to find the relation between volatility and these vectors. We use the FinBERT code gained from ProsusAI in Hugging Face (2021) to train the FinBERT model. Firstly, we attempt sentiment analysis of single-date text in 2025-10-10 and this model extracts the text list for the date, and then it performs text encoding via tokenizer to get the logits and transfer the probability of positive, negative or neutral by using the softmax. Finally, it match the built-in label mapping of the model dynamically which aims at avoiding hard coding, and output the three probabilities of each text and the final predicted emotion label. These codes are displayed by 1.py document. After training, we download the result of the vectors in 2.py shown in picture 1 and I find that the FinBERT model adopts 768-dimension vector which is too large for this project because our data volume only is about 2000, so we reduce the dimension of this vector to 128-dimension by pca (Hasan and Abdulazeez,2021).
&lt;img alt="The parameters of FinBERT model (partly)" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_768.png"&gt;&lt;/p&gt;
&lt;h4&gt;Y Selection&lt;/h4&gt;
&lt;p&gt;Additionally, Trump always posts several posts each day and we only have the daily volatility data so we should sum these vectors from text data in the same day to match the date of volatility shown by 7.py. Meanwhile, we merge the 128-dimension vector of text data and volatility data to prepare for machine learning. However, in this step, we original select the the difference of the volatility in two consecutive trading days as the dependent variable, and we assume the dependent variable y as the difference of volatility in two consecutive trading days exceed positive and negative 0.9 standard deviation (using the sd of the closest 40 trading days ) as the label 1, then we can use random forest to train the module, but in fact, we should attempt to adjust the value of sd to gain a suitable result based on the result of machine learning because there is no standard to show what range of fluctuation is considered abnormal, which means y could exceed positive or negative 0.5, 0.9 or other number and 128-dimension vector may also be large so we use for-loop to run the results based up on different values of dimension. Thus, there problem should be addressed while using random forest model.&lt;/p&gt;
&lt;h4&gt;Model Training&lt;/h4&gt;
&lt;p&gt;Random forest is a good model to address the overfitting problem so it could be used in the classification problem because it is suitable for high-dimension data with high robustness (Breiman,2001). We randomly select the train dataset and test dataset with ratio 7:3, and the independent variables are the 128-dimension vectors in each trading day and the dependent variable y defined before. And we define that the type 0 is the volatility of the second day is in range of the first day minus and add the sd with relevant coefficient, and the remaining are of type 1. To maximize the precision, we select n_estimator with 800 and shows the precision when reducing the dimensions of the vector from 3 to 64. We find that there is almost no difference when the dimension is above 21. Therefore, we only display the results with the dimension between 3 to 21. Afterwards, we adjust different numbers of the sd to run random forest model. Meanwhile, in order to use the precision and recall to measure the reliability of the model, the confusion matrix is used. Finally, we find that the best result shown in picture2 (11.py document) is when the dimension is 9 and y should exceed positive or negative 0.514 sd. We find that the recall and precision of type 0 and 1 are both higher than 50%, which shows that Trump’s posts have relationship with volatility in some degree. However, this result does not mean the next step that is our strategy will use the result since it depends on the profit gained based on the model. Therefore, we keep many types of labels with 21 different numbers of the coefficient to use further.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The result of random forest" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_RFout.png"&gt;&lt;/p&gt;
&lt;h2&gt;XGBoost Tree Model&lt;/h2&gt;
&lt;h4&gt;Feature Construction (X Variables)&lt;/h4&gt;
&lt;p&gt;In the XGBoost model, we constructed independent variables from a textual sentiment perspective, primarily employing three sentiment analysis methods: TextBlob, VADER, and FinBERT. Additionally, we introduced a feature based on text length.&lt;/p&gt;
&lt;p&gt;First, we used the TextBlob module to extract two continuous sentiment variables: Polarity and Subjectivity. Subsequently, the VADER sentiment analyzer calculates four sentiment scores: vader_neg, vader_neu, vader_pos, and vader_compound. Thus, TextBlob and VADER collectively provide six dictionary- and rule-based sentiment variables.&lt;/p&gt;
&lt;p&gt;The primary code for the TextBlob and VADER modules is as follows:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;textblob&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TextBlob&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;punkt_tab&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;averaged_perceptron_tagger&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;polarity_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;subjectivity_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;blob_par&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TextBlob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;polarity_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;blob_par&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;subjectivity_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;blob_par&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subjectivity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Polarity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;polarity_list&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Subjectivity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subjectivity_list&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.sentiment.vader&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SentimentIntensityAnalyzer&lt;/span&gt;
&lt;span class="n"&gt;analyzer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SentimentIntensityAnalyzer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;vader_neg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;vader_neu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;vader_pos&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;vader_compound&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;analyzer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity_scores&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;vader_neg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;neg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;vader_neu&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;neu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;vader_pos&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pos&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;vader_compound&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;compound&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Second, we introduce the pre-trained FinBERT model to predict sentiment classification probabilities for tweet texts, yielding probability outputs for three sentiment categories: positive (p_pos), negative (p_neg), and neutral (p_neu).&lt;/p&gt;
&lt;p&gt;The main code body is as follows:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tqdm.auto&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;

&lt;span class="n"&gt;model_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ProsusAI/finbert&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;use_safetensors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;finbert_score_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;text_col&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;max_length&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="n"&gt;texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;text_col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;texts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ne&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;idxs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;texts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;probs_all&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;texts&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;enc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;texts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="n"&gt;return_tensors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;truncation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;max_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;max_length&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;no_grad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;enc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;probs_all&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;probs_all&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probs_all&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;p_pos&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;p_neg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;p_neu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idxs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;p_pos&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;p_neg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;p_neu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probs_all&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;pos_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neg_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neu_id&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sent_dir&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;p_pos&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;p_neg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sent_abs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sent_dir&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;count_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[^A-Za-z0-9&amp;#39;]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;word_count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Based on this, we further construct two derived metrics:&lt;/p&gt;
&lt;p&gt;sent_dir (Sentiment Direction Indicator): Composed of the difference between positive sentiment probability and negative sentiment probability, p_pos−p_neg, used to characterize the overall directionality of text sentiment;&lt;/p&gt;
&lt;p&gt;sent_abs (Sentiment Intensity Indicator): The absolute value of the sentiment direction indicator, used to measure the degree to which text sentiment deviates from a neutral state.&lt;/p&gt;
&lt;p&gt;Here, &lt;code&gt;sent_dir&lt;/code&gt; emphasizes the positive or negative orientation of sentiment, while &lt;code&gt;sent_abs&lt;/code&gt; captures the intensity of emotional expression. We believe directional and intensity-based sentiment features may carry distinct predictive implications in financial contexts, hence incorporating both into the model.In summary, the final X variable comprises 12 dimensions: 6 from TextBlob and VADER, 5 from FinBERT (3 raw probability variables plus 2 derived variables sent_dir and sent_abs), and 1 text length variable word_count. These features will be uniformly input into the XGBoost model for training and evaluation in subsequent steps.&lt;/p&gt;
&lt;h4&gt;Feature Integration&lt;/h4&gt;
&lt;p&gt;During the modeling process, directly mapping multiple text features (X) from the same trading day to the same daily market outcome variable (Y) for training inevitably introduces structural bias. The root cause lies in the fact that multiple texts released within a single trading day often pertain to different events or themes, thereby reflecting divergent or even conflicting sentiment information. However, during the data construction phase, these texts are uniformly mapped to the same market volatility label. This issue significantly increases intra-sample heterogeneity, making it difficult for the model to identify which sentiment features truly hold explanatory or predictive value. Consequently, it weakens the model's stability and generalization capabilities, potentially introducing systematic estimation bias during training.&lt;/p&gt;
&lt;p&gt;Therefore, in subsequent modeling, we perform daily aggregation on text features from the same trading day. Specifically, for each sentiment variable corresponding to individual texts within the same day, we employ the mean as the primary aggregation method to characterize the overall sentiment level for that day.&lt;/p&gt;
&lt;p&gt;For the Subjectivity variable from TextBlob, aggregation is performed using the sum method. This approach aims to capture the cumulative degree of subjective expression within the day's texts—that is, the overall intensity of sentiment expression on the subjective dimension for the day, rather than the average level of individual texts. Since subjectivity reflects the prominence of emotional or stance expressions in the text, its cumulative effect is more likely to influence the market's overall reaction to that day's information. Therefore, we employ summation for integration on this dimension.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;df1&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;trade_date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;as_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;p_pos&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;p_neg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;p_neu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Polarity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Subjectivity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;VADER_compound&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;sent_dir&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;sent_abs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;word_count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;VADER_neg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;VADER_neu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;VADER_pos&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;vol&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;max&amp;#39;&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;Model Training&lt;/h4&gt;
&lt;p&gt;During the model training phase, we first perform a train-test split on the dataset. Given the pronounced time-series characteristics of the research subject, samples exhibit potential temporal autocorrelation. Employing a random split could potentially leak future information into the training set, thereby overestimating the model's actual predictive capability. Therefore, we abandon random sampling and instead partition the data based on temporal order: using earlier samples as the training set and later samples as the test set, thereby better approximating real-world prediction scenarios.&lt;/p&gt;
&lt;p&gt;Additionally, due to significant imbalance between positive (class 1) and negative (class 0) samples in the target variable, directly training the model risks classifiers favoring the majority class, thereby compromising recognition capabilities for the minority class. To mitigate this issue, we implemented oversampling on positive samples during training. This balanced the number of positive and negative samples in the training set, thereby enhancing the model's learning capacity and recognition effectiveness for minority class events.&lt;/p&gt;
&lt;p&gt;Since the objective of this project is to predict price spikes upward in VIXY for the next trading day (classified as the positive class, label=1), and the model output will be directly used to generate trading signals, we place greater emphasis on the effectiveness of positive class identification in our evaluation. Specifically, positive class predictions correspond to potential tradable opportunities, while a high number of false positives leads to unnecessary trades and stop-loss costs, ultimately eroding overall returns. Therefore, we set the core objective for model selection as follows: maximize positive class precision while ensuring the positive class recall meets the minimum requirement.&lt;/p&gt;
&lt;p&gt;The final model parameters (After Random Search) are:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;xgb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;XGBClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_estimators&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2350&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;subsample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;colsample_bytree&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;reg_lambda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;objective&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;binary:logistic&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;eval_metric&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;aucpr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_jobs&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;tree_method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hist&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The Output is: &lt;/p&gt;
&lt;p&gt;&lt;img alt="XGBoost Model Output" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_XGout1.png"&gt;&lt;/p&gt;
&lt;h2&gt;Model Prediciton Backtesting&lt;/h2&gt;
&lt;p&gt;After obtaining the best prediction results from Random Forest and XGBoost, we wrote the results back to the date labels and attempted to backtest them for trading. Since our prediction target is VIXY price surges, our trading strategy will focus on achieving a significantly high profit-to-loss ratio, disregarding win rate.&lt;/p&gt;
&lt;p&gt;Our final trading strategy employs a predictive signal-driven event-based (intraday to overnight) trading framework. First, the model is trained on historical data, with only the out-of-sample prediction results written back to the trading log to avoid look-ahead bias. When the model generates a valid prediction signal (y_pred) on a given trading day, the strategy initiates a position at the t+1 day's opening price (Open_{t+1}). Profit-taking and stop-loss levels are not fixed ratios but dynamically estimated based on the average spike up/down amplitude over the past 20 trading days to capture market volatility structure: the take-profit level is set at 4 times the historical average upward amplitude above the opening price, while the stop-loss level is set at 1 times the historical average downward amplitude below the opening price, creating an asymmetric risk-reward structure. During the holding period, if the lowest price on day t+1 first hits the stop-loss price, the position is closed via stop-loss exit. Otherwise, if the highest price reaches the take-profit price, the position is closed for profit. If neither is triggered, the position is closed at the t+1 closing price. At the same time, if both take-profit and stop-loss orders are triggered simultaneously, we prioritize executing the stop-loss strategy to stress-test our trading model. Final returns are measured by the yield relative to the opening price based on the actual exit price. &lt;/p&gt;
&lt;h4&gt;Backtesting Result(Random Forest):&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Random Forest Model Trade Backtesting Output" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_RFBTDT1.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Trade Backtesting Equity Curve for RF" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_RFBTDT2.png"&gt;&lt;/p&gt;
&lt;h4&gt;Backtesting Result(XGBoost):&lt;/h4&gt;
&lt;p&gt;&lt;img alt="XGBoost Model Trade Backtesting Output" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_BTDT1.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Trade Backtesting Equity Curve" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_BTDT2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Signal trade vs. Daily trade" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_BTDT3.png"&gt;&lt;/p&gt;
&lt;h2&gt;Experimental model: LSTM Neural Network Model&lt;/h2&gt;
&lt;h4&gt;Why LSTM？&lt;/h4&gt;
&lt;p&gt;Random Forest treats each trading day as an independent observation. However, both volatility and information flow are sequential: market uncertainty can accumulate over days, and tweet narratives can be persistent rather than one-off shocks. Therefore, we use Long Short-Term Memory (LSTM), a recurrent neural network designed to learn temporal dependencies in sequences.&lt;/p&gt;
&lt;p&gt;In our implementation, each observation is no longer “one day”, but a sequence window of past days’ embeddings, and the model predicts whether the current day is an abnormal volatility event.&lt;/p&gt;
&lt;h4&gt;Current Progress&lt;/h4&gt;
&lt;p&gt;Currently, we hold high expectations for the potential performance of LSTM models. Compared to traditional machine learning methods, LSTMs possess a structural advantage in capturing long-term temporal dependencies within time series data. This characteristic holds promise for significantly enhancing the modeling capability of target variables' dynamic evolution processes, thereby further improving predictive performance. However, uncertainties remain regarding the specific design of LSTM training workflows and the systematic tuning of key hyperparameters—such as network architecture, time window length, and regularization mechanisms. These aspects require further exploration through more in-depth experiments and validation in subsequent research.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Current Progress: LSTM" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_02_LSTMout.png"&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Yang, Y., Uy, M.C.S. &amp;amp; Huang, A. (2020). FinBERT: A pretrained language model for financial communications. arXiv preprint arXiv:2006.08097. &lt;/p&gt;
&lt;p&gt;[2] ProsusAI. (2020). FinBERT. Hugging Face. &lt;a href="https://huggingface.co/ProsusAI/finbert"&gt;ProsusAI&lt;/a&gt; Accessed 21 January 2026.&lt;/p&gt;
&lt;p&gt;[3] Hasan, B.M.S. and Abdulazeez, A.M. (2021) 'A review of principal component analysis algorithm for dimensionality reduction', Journal of Soft Computing and Data Mining, 2(1), pp.20–30.&lt;/p&gt;
&lt;p&gt;[4] Breiman, L. (2001) 'Random forests', Machine Learning, 45(1), pp.5–32.&lt;/p&gt;</content><category term="Blog"></category><category term="volatility"></category><category term="sentiment"></category></entry><entry><title>When Trump Tweets, Markets Listen? (by Group "PoliVoli")</title><link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/when-trump-tweets-markets-listen-by-group-polivoli.html" rel="alternate"></link><published>2026-01-09T21:49:00+08:00</published><updated>2026-01-09T21:49:00+08:00</updated><author><name>MFIN7036 Students 2025</name></author><id>tag:buehlmaier.github.io,2026-01-09:/MFIN7036-student-blog-2025-12/when-trump-tweets-markets-listen-by-group-polivoli.html</id><summary type="html">&lt;p&gt;By Group "PoliVoli"&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Predicting stock prices has always been difficult because so many factors can influence market movements. Global events, policy changes and even statements by public figures can cause markets to move dramatically and without warning. For example, when the US raises interest rates, the stock market tends …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "PoliVoli"&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Predicting stock prices has always been difficult because so many factors can influence market movements. Global events, policy changes and even statements by public figures can cause markets to move dramatically and without warning. For example, when the US raises interest rates, the stock market tends to fall because tighter money causes investors to move their money to the banks, which reduces their investment in stocks. Or if an investor misjudges the timing of an interest rate hike, he could face huge losses. Sudden political comments can be equally devastating. A single dramatic comment from a leader can trigger dramatic market turmoil overnight. This study will focus on Trump's personal social media dynamics to explore whether there is a correlation between his emotional rhetoric and increased stock market volatility?&lt;/p&gt;
&lt;p&gt;Our choice to focus on market volatility has particular considerations. Trump's rhetoric may positively or negatively impact specific firms or industries, but it is extremely difficult for the public to predict whether his next statement will praise or bash a particular firm. For outsiders, his future rhetoric is inherently random. And volatility precisely reflects market anxiety and uncertainty. If it turns out that Trump's subjective and emotional tweets tend to trigger higher volatility (regardless of whether the market rises or falls), this finding in itself could be enough to help investors take precautionary measures ahead of time, such as hedging with options or adjusting their portfolios in response to market shocks. Our goal is to reveal patterns of correlation between Trump's social media sentiment and the Chicago Board Options Exchange Volatility Index (Cboe VIX).&lt;/p&gt;
&lt;p&gt;To address this problem, we plan to adopt a three-stage program (shown in Figure 1). First, we will collect and preprocess data: social media posts (text data) of Trump and their corresponding daily VIX index values from 2017-2025. Second, natural language processing (NLP) techniques are applied to quantify the sentiment tendency (positive/negative) and subjectivity (personal opinion vs. factual statements) of each post. Finally, we input these sentiment indicators into the model to verify whether we can predict high volatility versus low volatility trading days. At this stage, we tentatively decided to try a Random Forest model for categorical analysis because our target variable (whether volatility spikes or not) is essentially a binary event, and we believe that robust integration methods such as Random Forests can capture non-linear relationships and interactions in the data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1. Project flowchart" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_01_FlowChart.png"&gt;&lt;/p&gt;
&lt;h2&gt;Data Collection – Trump’s Posts and Market Volatility Data&lt;/h2&gt;
&lt;p&gt;Why Trump's tweets and posts from 2017 to 2025? Because we needed a complete dataset covering Trump's long-term personal rhetoric, both during his time in office (2017 to January 2021, his active tweeting phase) and after he left office (switching to the Truth Social platform). This time span both shows the trajectory of the evolution of his market influence and ensures that we have a sufficient observational sample of posts and market reactions. But getting the data wasn't easy. Initially we considered crawling tweets directly, but API access to Twitter has been cut off, and the Truth Social platform itself presents challenges as there is no official API and a VPN is required to access it in our region. To access recent data, we turned to open-source archives already compiled by other researchers.&lt;/p&gt;
&lt;p&gt;Twitter archive: we found that user MarkHershey has created a complete archive of Trump tweets on GitHub (covering all tweets from 2017-2021 in office). The resource not only saves a lot of time, but also bypasses the Twitter API block. The archive contains the timestamp and full content of each tweet, exactly the core data needed for text analytics. We pay particular attention to tweets from the presidency, when markets were extremely sensitive to his rhetoric - remember the sharp swings triggered by his references to trade tariffs or specific companies? It is these events that motivate our research.&lt;/p&gt;
&lt;p&gt;Truth Social Platforms Archive: for Trump's post-2022 tweets, we found a GitHub project created by Matt Stiles that automatically updates an archive of Trump's postings on truth social platforms. The repository is complete with raw data from the Truth Social Platform from its launch (late 2021/early 2022) to October 25, 2025. The choice to focus on the truth social platforms is due to the fact that since Trump was blocked from Twitter in January 2021, the platforms have been the only outlet for his public statements. In order to continue the analysis beyond 2021, these posts are critical. Employing archived data is indeed expedient, as the original crawl script expires in late 2025 and requires a US IP address to attempt to run, whereas the owner of the repository has kept the required data intact. As a result, by integrating the Twitter and Truth in Social datasets, we ended up with a complete record of Trump's public postings from early 2017 through late 2025 - a solid foundation for subsequent analysis.&lt;/p&gt;
&lt;p&gt;Market Volatility (VIX) Data: to measure stock market volatility, we chose the Chicago Board Options Exchange Volatility Index (VIX). The VIX was chosen because it is the most widely used indicator of market volatility and is often referred to as the “panic index”. The index is published by the Chicago Board Options Exchange and is based on the price of S&amp;amp;P 500 options to project market participants' expectations of volatility over the next 30 days. In short, a spike in the VIX means that traders expect the market to move violently (up or down). We obtained daily VIX historical data for 2017-2025 from an official source (Investing.com Historical Data Board). From this, we obtained the daily VIX closing values and the opening, high and low prices for each trading day. In this context, matching the VIX data time period with the Trump tweet data time period is crucial, as it ensures that on a given date, each Trump tweet, or no tweet, corresponds to a quantitative indicator of market volatility.&lt;/p&gt;
&lt;p&gt;The key question now is how to define modelable “volatility events” or high volatility. Since the VIX is a continuous value on a daily basis, we decided to use a binary categorization: “high volatility days” versus “low/normal volatility days”. In order to set the thresholds objectively, we analyzed the distribution of VIX values during the study period. We chose the VIX mean and median as potential cut-off points. Why? The median naturally divides the high and low halves of volatility observations, while the mean (which can be affected by sharp fluctuations) provides an alternative reference benchmark.&lt;/p&gt;
&lt;p&gt;By categorizing trading days as “high volatility” (above threshold) and “low volatility” (below threshold), we reduce the forecasting problem to a binary outcome. This approach fits our application scenario because investors mainly need to determine whether volatility is abnormally high or not, rather than precisely predicting the exact magnitude of the change in the VIX index. Moreover, the use of binary variables is suitable for categorical models, which are more resistant to outliers than regression models, because extreme spikes in the VIX do not distort the model, but are only treated as “high volatility” events like other high volatility days. To avoid overly influencing the results by threshold selection preferences, we will test both the mean and the median as threshold rules (the median may be slightly more conservative if the distribution is skewed).&lt;/p&gt;
&lt;p&gt;At the same time, “abnormally high” volatility can be defined not only by the VIX level, but also by short-term changes in volatility around the time of Trump's tweets. Therefore, we examine Δ-1, Δ+1, and Δ+2 change indicators. In addition to the dates by volatility level, we also calculate multiple indicators of volatility change around the date of Trump's tweet:&lt;/p&gt;
&lt;p&gt;Δ-1: the change in the VIX from the previous trading day to the tweet release date&lt;/p&gt;
&lt;p&gt;Δ+1: the magnitude of the change from the day the tweet was released to the next trading day&lt;/p&gt;
&lt;p&gt;Δ+2: the magnitude of the change from the day of the tweet to two trading days later.&lt;/p&gt;
&lt;p&gt;The inclusion of these metrics is intended to capture the immediate impact of Trump's comments before and after, as his tweets are posted at irregular times: sometimes before the market opens, sometimes in the middle of the afternoon, sometimes late at night. A single day's VIX close may not reflect the full picture - the market may have shown nervousness the day before (reflected in Δ-1) or the market's reaction may have been delayed a day in showing up (reflected in Δ+1 or even Δ+2). By measuring the change in volatility before and after the tweets, we aim to more accurately distinguish the impact of Trump's comments from other unrelated market events. For example, if the VIX had been climbing continuously prior to Trump's remarks, the increase could have stemmed from other news; the inclusion of Δ-1 as a characteristic quantity avoids misattributing it to Trump. Similarly, Δ+1 and Δ+2 enable us to observe whether volatility continued to spike or leveled off in the days following the remarks, which could be a sign that the market digested his remarks. We also extract the daily intraday high/low range and the absolute difference between the open and close prices. These two indicators reflect the intensity of intraday volatility - sometimes the VIX closes weakly, but violent intraday jolts still indicate market turbulence. Incorporating an intraday volatility indicator is critical, as Trump's midday tweets can trigger a brief intraday jump followed by a pullback; even if the net movement is small, the high-low spreads can still capture this type of volatility. By using a combination of these volatility indicators, we can define or identify “volatility events” in multiple dimensions.&lt;/p&gt;
&lt;p&gt;At the practical level, we use simple heuristics to initially label the classification targets and then evaluate the robustness of the different definitions. Under the delta-based labeling rule, if the VIX rises (i.e., Δ0 or Δ+1 is positive) within a short period of time before and after a tweet, it is labeled as 1 (high volatility). This is the initial definition used for current model training, with the intuitive logic that there may be correlation if volatility rises in a short period of time after the release of their remarks. Meanwhile, under the threshold labeling rule, which marks a high volatility day when the VIX is above the threshold and a low volatility day otherwise, we will test both mean and median thresholding schemes. We recognize that these are still rough measures: not all volatility rises stem from Trump, and sometimes his rhetoric may dampen declines rather than trigger rises. Future iterations may require VIX volatility to exceed specific thresholds or be compared to average market performance to improve scientific rigor. However, at this stage these simple rules provide a practical starting point for categorization and allow us to compare the results of both the median/mean threshold approach and the delta-based event definition.&lt;/p&gt;
&lt;h2&gt;Data Preprocessing – Cleaning and Aligning the Data&lt;/h2&gt;
&lt;p&gt;With data in hand, the next step was cleaning it up and aligning Trump’s posts with the correct trading days. &lt;/p&gt;
&lt;h4&gt;Cleaning the Text Data&lt;/h4&gt;
&lt;p&gt;Our original data files contain a large number of strange characters (called "mojibake"). This is due to text encoding errors (e.g., UTF-8 text being incorrectly interpreted as Latin-1). We need Trump's posts to appear in the correct word form, as any garbled characters will interfere with our subsequent sentiment analysis. So, fixing text encoding was priority one. We wrote a custom fix_mojibake function to systematically detect these weird character patterns and convert them to the proper text. Meanwhile, Trump’s posts contain a lot of special characters (emojis, fancy quotes, dashes, etc.), and a robust fix helps ensure we don’t lose or distort information like emphasis or sarcasm indicated by punctuation. Our function first tries to re-interpret the text in the correct encoding (by encoding as Latin-1 and decoding as UTF-8) and then falls back on a dictionary of known bad-to-good character mappings. This two-step approach was chosen because some errors could be fixed by re-encoding, while others needed explicit replacement. It took some time to refine (we encountered many edge cases), but the result was a near-complete recovery of the original post content. &lt;/p&gt;
&lt;p&gt;After correctly decoding the text, we performed standard text cleanup. Sometimes, Trump's social media posts included retweets or "retweet the truth" (starting with "RT"), user mentions, and URLs pointing to articles or videos. We systematically removed this content. The rationale is that we only focus on Trump's personal statements and emotions—sharing Fox News links or retweeting others' tweets doesn't reflect his personal emotional tone, and URLs and mentions don't contain meaningful emotional information. We used a URL extraction library to remove links and all content starting with "RT." We performed this on both Truth Social and Twitter data. This cleanup effectively reduces noise: extra words or symbols irrelevant to emotion can interfere with our sentiment model (e.g., "http" or "t.co" might be considered neutral). By cleansing this content, we ensured that our sentiment algorithm focused on the actual information content. We also removed posts that were completely blank after removing these elements.&lt;/p&gt;
&lt;p&gt;One tricky issue we encountered was CSV formatting. The Truth Social CSV had 8 columns per row (like timestamp, content, likes, etc.), but many posts contained commas or line breaks that were not handled properly, causing some posts to spill into extra columns. This misalignment meant that a post’s content might get split, and the numeric fields (like replies_count, likes_count) would shift columns, making the data inconsistent. We noticed that some rows were missing values where they should have been numbers, or contained impossible values (e.g., a date field appearing in a likes column). Therefore, we wrote a fix: for each row, we checked if a field that should have contained a number actually did. If not, we assumed it was due to a data misalignment caused by commas in the content. We then re-merged the data until it aligned with the numeric field. This reconstructed the complete content of each post and correctly adjusted the counts. After fixing the Truth Social data structure, doing the same for Twitter was much easier: Twitter content sometimes contained commas/newlines in columns where numbers shouldn't have been, but since there were no important columns after the content (perhaps only the ID), we could simply merge all the trailing columns into a single content field.&lt;/p&gt;
&lt;p&gt;After data cleaning, we obtained two cleaned datasets of Trump posts, each containing timestamps and clean text content suitable for sentiment analysis. We then decided to merge the two data sources (Twitter and Truth Social) to unify the timeline of postings. This simplifies future alignment with VIX data—we only need to handle consecutive date series. Furthermore, this approach ensures data consistency should we need to build features such as "Was a post published today?". During the merging process, we were very careful, retaining all fields and logging platform information where necessary, although the platform may not be important for the analysis since all content represents Trump's voice.&lt;/p&gt;
&lt;h4&gt;Aligning Posts with Trading Days&lt;/h4&gt;
&lt;p&gt;Since VIX data is indexed by trading days (no data on weekends or holidays), each value actually represents the volatility index as of the market close on that day (VIX closes at 3:15 PM Central Time). However, Trump's posting times are uncertain, requiring us to map each post to the correct trading day. For example, a post late on Sunday, May 3rd should be associated with the VIX index on Monday, May 4th (since markets are closed on Sundays and would only react on Monday).&lt;/p&gt;
&lt;p&gt;We verified the timestamps of posts across both social media platforms by cross-checking and consulting a friend in the U.S., as these platforms default to UTC time. Subsequently, we converted all timestamps to Central Time (CT), the time zone used for VIX data. Additionally, due to market closing times, any posts published after 3:15 PM CT should be attributed to the next trading day's volatility, as the market had no opportunity to react that day. Conversely, posts published before 3:14 PM CT fall within the influence of that day's trading session. To address this, we shift timestamps forward by 8 hours and 45 minutes. This ensures all posts published after 3:15 PM CT are timestamped after midnight on the following day. After this time adjustment, we can directly use the date portion of the timestamp as the “effective trading day.”&lt;/p&gt;
&lt;p&gt;We must also account for market closures. If the next date falls on a Saturday or a holiday like July 4th, simply shifting to the next calendar date is insufficient. To address this, we extracted the NYSE trading calendar (using the pandas_market_calendars library) to obtain a list of all valid trading days. Then, for each post's tentative trading date, if it falls on a non-trading day, we shift it to the next actual trading day.&lt;/p&gt;
&lt;p&gt;This alignment is vital for the integrity of our analysis. It means when we later aggregate Trump’s posts or sentiment by day, and compare to that day’s VIX movement, we’re doing it in the right way. Without it, we might mistakenly link a Sunday night fiery post to the previous Friday’s market (which had no knowledge of it), or we’d miss the effect on Monday.&lt;/p&gt;
&lt;h2&gt;Sentiment Analysis – Measuring Emotion and Subjectivity&lt;/h2&gt;
&lt;p&gt;With clean, time-aligned text data, we can move on to the natural language processing stage: extracting sentiment signals from Trump's statements. We hypothesize that when Trump's tone is highly emotional or subjective, it may create uncertainty or excitement in the market, leading to increased market volatility. To test this hypothesis, we need to quantify two aspects of each article: polarity (positive/negative degree) and subjectivity. We chose two methods, TextBlob and VADER, to calculate these metrics. Here are the reasons for our choice of these two methods and their respective advantages:&lt;/p&gt;
&lt;h4&gt;TextBlob&lt;/h4&gt;
&lt;p&gt;We chose TextBlob because it is a well-known and easy-to-use library that provides ready-made sentiment analysis capabilities. It works by breaking down text into words and phrases and using a lexicon (from a library included in NLTK for part-of-speech tagging) combined with some basic grammatical rules. For each post, TextBlob returns two key values: polarity, ranging from -1 to 1, where -1 represents very negative, 0 represents neutral, and +1 represents very positive; and subjectivity, ranging from 0 to 1, where 0 represents a purely objective statement and 1 represents highly subjective/opinion-based. Highly subjective posts indicate that Trump has mixed personal opinions or emotions into the post, which could create more uncertainty among investors and potentially lead to greater market volatility. TextBlob's strengths lie in its high transparency and fast computation speed. However, we also recognized that its use of a general English lexicon, word-by-word scoring, and an overly linear sentiment weighting algorithm could lead to misunderstandings of complex contexts. As our results show, in the Polarity score, Trump's sentiment approximates a normal distribution, but in the Subjectivity score, Trump's posts tend to be more subjective. This suggests that the simple summation method of TextBlob may not be accurate when dealing with long texts. We may need a more complex model to transform long tweets like Trump's, which are characterized by strong sentiment and strong subjectivity. Therefore, we subsequently tried to supplement it with VADER.
&lt;img alt="Figure 2. Polarity &amp;amp; Subjectivity Distribution" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_01_Polarity_Subjectivity.png"&gt;&lt;/p&gt;
&lt;h4&gt;VADER (Valence Aware Dictionary for Sentiment Reasoning)&lt;/h4&gt;
&lt;p&gt;We attempted to supplement TextBlob after believing its limitations might affect the final data transformation. Designed specifically for short, informal texts like tweets, VADER addresses the logical inversions between multiple words compared to TextBlob's single-word scoring. For example, if "not" precedes "good," VADER reverses the overall sentiment. Therefore, we believed it should perform better on more emotionally charged texts posted by Trump in tweets or Truth.&lt;/p&gt;
&lt;p&gt;VADER breaks down sentiment into positive, negative, and neutral components and constructs a weighted summation of a comprehensive sentiment score from -1 to 1 (similar to TextBlob's Polarity). We hoped that by using VADER, we could more accurately capture this sentiment and reflect readers' (and investors') potential reactions to Trump's tone.&lt;/p&gt;
&lt;p&gt;In terms of results, VADER provided what we considered a more comprehensive score. Because platforms like Twitter and Truth tend to elicit more emotional responses, we believe Trump's posts will be more emotionally biased. This is difficult to discern in the scores provided by TextBlob. However, in the Compound scores provided by VADER, we can clearly observe a more even distribution of sentiment across the axis in Trump's tweets, a result we consider more reliable.
&lt;img alt="Figure 3. VADER Compound Distribution" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/PoliVoli_01_VADER_Compound.png"&gt;&lt;/p&gt;
&lt;h2&gt;Modeling Volatility from Sentiment – Our Strategy&lt;/h2&gt;
&lt;p&gt;Finally, let's discuss how to link the sentiment of Trump's posts with market volatility data. As mentioned earlier, we model it as a classification problem: given the sentiment characteristics of Trump's posts on a given day, can we predict whether market volatility will surge that day? We choose classification (high volatility vs. low volatility) rather than predicting a precise VIX value for two reasons. First, investors want a quick warning of market turmoil; a specific point is difficult to predict and can be misleading. Second, Trump's impact on the market may be non-linear, meaning volatility may only spike when the uncertainty brought by a Trump post exceeds extreme levels. Regression might weaken this effect, while classification is better suited for this task.&lt;/p&gt;
&lt;p&gt;Our next step will be trying the random forest classifier. As a relatively well-understood machine learning model, random forests can capture the non-linear relationships and interactions between sentiment features (e.g., the combination of high negative sentiment and high subjectivity). Furthermore, due to the complexity of the transformation between sentiment and volatility, it is difficult to fit the data using linear models, which would reduce the interpretability of our results. However, in this task, we want to avoid using black-box models as much as possible. Random forests provide a feature importance metric, allowing us to explain which sentiment signals (e.g., subjectivity and negative sentiment) are most important for predicting market volatility, thereby improving the interpretability of the results.&lt;/p&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;In the next step, the real test will be to train the model and verify that our intuition holds true: do Trump's emotional outbursts coincide with, or even predict, days of sharp market swings? A follow-up report will present model performance, interesting patterns (e.g., whether highly subjective content on the Truth Social platform correlates with sharp market shocks during 2022-2023), and other possible unexpected findings. If the results do not meet expectations, we will explore options for improvement: e.g., adding contextual information (are the posts about economic policy or trivia?), or using more advanced techniques such as BERT. or using more advanced natural language processing techniques such as BERT to capture context.&lt;/p&gt;
&lt;p&gt;By articulating our thinking at each step, we hope to clearly demonstrate that all choices are based on domain knowledge or practical constraints, and that each decision brings value to solving the puzzle of the relationship between Trump and market volatility. Stay tuned for the final results, which will reveal how a single individual's tweets can move markets, and what exactly is their impact?&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] stiles, "trump-truth-social-archive," GitHub,  &lt;a href="https://github.com/stiles/trump-truth-social-archive/tree/main/data"&gt;Github-trump-truth&lt;/a&gt; (accessed Jan. 9, 2026).&lt;/p&gt;
&lt;p&gt;[2] MarkHershey, "CompleteTrumpTweetsArchive," GitHub,  &lt;a href="https://github.com/MarkHershey/CompleteTrumpTweetsArchive"&gt;TrumpTweetsArchive&lt;/a&gt; (accessed Jan. 9, 2026).&lt;/p&gt;
&lt;p&gt;[3] "TextBlob Documentation," Read the Docs, &lt;a href="https://textblob.readthedocs.io/en/dev/"&gt;TextBlob&lt;/a&gt; (accessed Jan. 9, 2026).&lt;/p&gt;
&lt;p&gt;[4] "Natural Language Toolkit (NLTK)," NLTK Project,  &lt;a href="https://www.nltk.org/"&gt;NLTK Project&lt;/a&gt; (accessed Jan. 9, 2026).&lt;/p&gt;
&lt;p&gt;[5] "VADER Sentiment Documentation," Read the Docs,  &lt;a href="https://vadersentiment.readthedocs.io/en/latest/index.html"&gt;VADER&lt;/a&gt; (accessed Jan. 9, 2026).&lt;/p&gt;</content><category term="Blog"></category><category term="volatility"></category><category term="sentiment"></category></entry></feed>