<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="MFIN7036 Students 2025" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Group Lighthouse Securities, Reflective Report, " />

<meta property="og:title" content="FOMC vs. ECB - How Minutes Influence Market Volatility (by Group &#34;Lighthouse Securities&#34;) "/>
<meta property="og:url" content="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/fomc-vs-ecb-how-minutes-influence-market-volatility-by-group-lighthouse-securities.html" />
<meta property="og:description" content="By Group &#34;Lighthouse Securities&#34; Lighthouse Securities: The trusted light for your financial journey 1. Introduction Welcome back to our series on central bank communications and market dynamics. In our previous blogpost, we laid the groundwork for our project, exploring the ideation process and the technical hurdles of scraping high-quality textual …" />
<meta property="og:site_name" content="MFIN7036 Student Blog 2025" />
<meta property="og:article:author" content="MFIN7036 Students 2025" />
<meta property="og:article:published_time" content="2026-01-21T12:00:00+08:00" />
<meta name="twitter:title" content="FOMC vs. ECB - How Minutes Influence Market Volatility (by Group &#34;Lighthouse Securities&#34;) ">
<meta name="twitter:description" content="By Group &#34;Lighthouse Securities&#34; Lighthouse Securities: The trusted light for your financial journey 1. Introduction Welcome back to our series on central bank communications and market dynamics. In our previous blogpost, we laid the groundwork for our project, exploring the ideation process and the technical hurdles of scraping high-quality textual …">

        <title>FOMC vs. ECB - How Minutes Influence Market Volatility (by Group &#34;Lighthouse Securities&#34;)  · MFIN7036 Student Blog 2025
</title>
        <link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="MFIN7036 Student Blog 2025 - Full Atom Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/"><span class=site-name>MFIN7036 Student Blog 2025</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://buehlmaier.github.io/MFIN7036-student-blog-2025-12
                                    >Home</a>
                                </li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html">Categories</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html">Tags</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/fomc-vs-ecb-how-minutes-influence-market-volatility-by-group-lighthouse-securities.html">
                FOMC vs. ECB - How Minutes Influence Market Volatility (by Group "Lighthouse Securities")
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>By Group "Lighthouse Securities"</p>
<blockquote>
<p><strong>Lighthouse Securities</strong>:
The trusted light for your financial journey</p>
</blockquote>
<h2>1. Introduction</h2>
<p>Welcome back to our series on central bank communications and market dynamics. In our previous blogpost, we laid the groundwork for our project, exploring the ideation process and the technical hurdles of scraping high-quality textual data from both the Federal Reserve (FED) and the European Central Bank (ECB). We also established our data sources for market volatility across the US and European markets.</p>
<p>With the raw data safely in our hands, we move from collection to the core of our technical pipeline. In this post, we will dive into the nuances of data cleaning, structuring, and the initial natural language processing (NLP) techniques we are employing to turn dense policy accounts into quantifiable financial sentiment.</p>
<h2>2. Data Cleaning and Structuring</h2>
<p>When we first moved from data collection to data cleaning, we assumed this would be a relatively mechanical step — removing HTML tags, fixing encoding issues, and preparing the text for NLP models.In practice, this stage turned out to be one of the most time-consuming and conceptually challenging parts of the entire project.</p>
<p>The main difficulty was not technical, but institutional.
FOMC and ECB minutes differ not only in length, but also in structure, writing style, and—most importantly—timing.</p>
<p>A mistake we initially made was to align market data directly with the release dates of the minutes.
This created a subtle form of look-ahead bias, as the market often reacts to the underlying meeting information before the official publication.</p>
<p>Below is a simplified version of the cleaning logic we eventually settled on:
In hindsight, this stage taught us that in text-based financial research, data cleaning is not a preprocessing step, but an economic modeling decision.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Core cleaning stages</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">remove_html_tags</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">fix_encoding</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>  <span class="c1"># NFKC normalization</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">remove_names_and_titles</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>  <span class="c1"># Powell, Lagarde, etc.</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">extract_substantive_content</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>  <span class="c1"># Skip attendance sections</span>
</code></pre></div>

<p>After deduplication, we retained <strong>340 documents</strong> (ECB: 80, FOMC: 260) spanning 33 years. The two corpora differ significantly—FOMC minutes average 34,261 words versus ECB's 7,691 words.</p>
<p><img alt="Comparative Statistics" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/Lighthouse-Securities_02_comparative_stats.png"></p>
<h2>3. Natural Language Processing Techniques</h2>
<h3>3.1 Cosine Similarity</h3>
<p>The first thing we needed to do in both Approach 1 and Approach 3 was the embedding of the minutes. So we looked for models that could provide us with high-quality embeddings for our text data. </p>
<p>And we found BERT-based models are quite popular in the academic community for various NLP tasks, including text classification and sentiment analysis. But the limits of BERT actually pushed us to explore more advanced models like <code>Qwen3-Embedding-8B</code>, which has less limitations on the input text length and can handle longer documents more effectively.</p>
<p>But when we actually started to try to run the model on the Kaggle platform, we found that the model was quite resource-intensive and required a lot of computational power to run efficiently. So we had to turn to <strong>OpenRouter</strong>, which provided us with an API to access the Qwen3-Embedding model without needing to run it locally.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">base_url</span><span class="o">=</span><span class="n">OPENROUTER_BASE_URL</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">API_KEY</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">embedding_with_usage</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;qwen/qwen3-embedding-8b&quot;</span><span class="p">):</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">extra_headers</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;HTTP-Referer&quot;</span><span class="p">:</span> <span class="s2">&quot;https://github.com/Lighthouse-Securities/FOMC-Analysis&quot;</span><span class="p">,</span>
            <span class="s2">&quot;X-Title&quot;</span><span class="p">:</span> <span class="s2">&quot;Central Bank Analysis&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
        <span class="n">encoding_format</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">,</span>
        <span class="n">extra_body</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;usage&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;include&quot;</span><span class="p">:</span> <span class="kc">True</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input tokens:&quot;</span><span class="p">,</span> <span class="n">embedding</span><span class="o">.</span><span class="n">usage</span><span class="o">.</span><span class="n">prompt_tokens</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total tokens:&quot;</span><span class="p">,</span> <span class="n">embedding</span><span class="o">.</span><span class="n">usage</span><span class="o">.</span><span class="n">total_tokens</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost:&quot;</span><span class="p">,</span> <span class="n">embedding</span><span class="o">.</span><span class="n">usage</span><span class="o">.</span><span class="n">cost</span><span class="p">,</span> <span class="s2">&quot;credits&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">embedding</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">embedding</span><span class="o">.</span><span class="n">model_extra</span><span class="p">[</span><span class="s1">&#39;error&#39;</span><span class="p">][</span><span class="s1">&#39;message&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="kc">None</span>
</code></pre></div>

<p>Then we used the <code>Qwen3-Embedding</code> model to generate embeddings for each of the FOMC and ECB minutes. During the embedding, we found that even though the model could handle longer texts, some of the minutes were still too long to fit into a single input. So we had to truncate some of the minutes to fit within the model's input limits.</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">minutes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="o">...</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[:</span><span class="mi">128000</span><span class="p">]</span>
    <span class="o">...</span>
</code></pre></div>

<h3>3.2 LLM Classification</h3>
<p>When setting out on our analysis using Large Language Models (LLMs), we were planning to use state-of-the-art (SOTA) models for the best results. However, we quickly encountered a significant hurdle; being based in Hong Kong. Access to leading models from OpenAI, Google (<em>Gemini Flash 3.0 was our original choice</em>), and Anthropic was restricted due to regulatory constraints. This limitation forced us to pivot our approach and explore alternative models available through <strong>OpenRouter</strong>.</p>
<p>We aimed to keep costs low while still achieving reliable performance, so we looked for the best freely accessible model on OpenRouter. After evaluating several options, we settled on <strong>Xiaomi's Mimo v2 Flash</strong> model. While not as advanced as some of the restricted models, Mimo v2 Flash provided a good balance between accessibility and performance for our sentiment analysis tasks. The models are easy to use with only a few lines of code:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">base_url</span><span class="o">=</span><span class="n">OPENROUTER_BASE_URL</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">API_KEY</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>
</code></pre></div>

<p>We chose a temperature of 0.1 to ensure more deterministic outputs, which is crucial for consistent sentiment scoring. In our testing, the results are still not fully deterministic, but the variance is minimal enough to not significantly impact our regression analyses. After running the LLM classification on both FOMC and ECB minutes, we extracted key features such as sentiment, hawkishness, and inflation concern. These features were then integrated into our regression models to assess their impact on market volatility.</p>
<p>Creating a correct and, <em>ideally</em>, significant regression model is always challenging. We go through many iterations of finding the correct control variables, assessing multicollinearity, handling heteroscedasticity, and other statistical pitfalls. In our analysis, both GDP and CPI had to be dropped due to multicollinearity issues (both for FED and ECB). To deal with heteroscedasticity (tested through a Breusch-Pagan and White test), we used robust standard errors (HC3) throughout our regression analyses.</p>
<h3>3.3 Neural Embeddings</h3>
<p>In our approach 3, we have two models. The first model, which we called it the baseline model, is quite simple. We just feed the text vectors to feed forward neural network with two hidden layers. On the top of this baseline model, we would like to add some market variables as input, for example the FFR, GDP etc., since an intuitive thinking is that those market variables would also have great influence on the following stock market volitility. But here comes the question: what is a good way to combine the text vectors and market variables? A simple method is that we could concatenate the market variables after the text vecters directly, by which we create a new vector and feed to the FFN. The structure of this simple model is as follow:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="k">class</span> <span class="nc">easy_fusion_FNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_dim</span><span class="p">,</span> <span class="n">market_dim</span><span class="p">,</span> <span class="n">fusion_text_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">fusion_mkt_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">combine_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">drop_out</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            text_dim: the dimensions of text;</span>
<span class="sd">            markrt_dim: the dimensions of the market variables;</span>
<span class="sd">            fusion_text_dim: the numble of neurons in the hidden layer in the sub NN for text;</span>
<span class="sd">            fusion_mkt_dim: the numble of neurons in the hidden layer in the sub NN for market  variables;</span>
<span class="sd">            combine_dims: the number of neurons in combine layer;</span>
<span class="sd">            out_num: the number of output layer, since we are doing binary classification, here the out_put=1 defaulty;</span>
<span class="sd">            drop_out: value of the dropout() attribute;</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">easy_fusion_FNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">combine_fusion_dim</span> <span class="o">=</span> <span class="n">text_dim</span><span class="o">+</span><span class="n">market_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fusion_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">combine_fusion_dim</span><span class="p">,</span> <span class="n">combine_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_out</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">combine_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">combine_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_out</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)</span>
        <span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span> <span class="p">,</span> <span class="n">mkt</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">text</span><span class="p">,</span> <span class="n">mkt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_net</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
</code></pre></div>

<p>Theoretically speaking, the disadvantage of this model is that text vectors and market data are directly mixed, and the model may struggle to distinguish between two different types of data and may not adequately learn the characteristics of both data. So, we made some adjustments to the model and created a new model, which we called fusion model. The key is that we first used two sub-neural networks to process the text vectors and market variables respectively. Then, we concatenated the outputs of the two sub-networks as the input for the next hidden layer. In this way, the two sub-networks can respectively learn the text features and market features. But the weakness is that the complexity of the model is much more higher which may makes model doen't fit well to our experiment due to the less amount of data. The structure is as follow:</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">fusion_FNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_dim</span><span class="p">,</span> <span class="n">market_dim</span><span class="p">,</span> <span class="n">fusion_text_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">fusion_mkt_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">combine_dims</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">drop_out</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">            text_dim: the dimensions of text;</span>
<span class="sd">            markrt_dim: the dimensions of the market variables;</span>
<span class="sd">            fusion_text_dim: the numble of neurons in the hidden layer in the sub NN for text;</span>
<span class="sd">            fusion_mkt_dim: the numble of neurons in the hidden layer in the sub NN for market  variables;</span>
<span class="sd">            combine_dims: the number of neurons in combine layer;</span>
<span class="sd">            out_num: the number of output layer, since we are doing binary classification, here the out_put=1 defaulty;</span>
<span class="sd">            drop_out: value of the dropout() attribute;</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">fusion_FNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">text_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">text_dim</span><span class="p">,</span> <span class="n">fusion_text_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_out</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">market_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">market_dim</span><span class="p">,</span> <span class="n">fusion_mkt_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_out</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">combine_fusion_dim</span> <span class="o">=</span> <span class="n">fusion_mkt_dim</span> <span class="o">+</span> <span class="n">fusion_text_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fusion_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">combine_fusion_dim</span><span class="p">,</span> <span class="n">combine_dims</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">drop_out</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)</span>
        <span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span> <span class="p">,</span> <span class="n">mkt</span><span class="p">):</span>
        <span class="n">subNN_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_net</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">subNN_mkt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">market_net</span><span class="p">(</span><span class="n">mkt</span><span class="p">)</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">subNN_text</span><span class="p">,</span> <span class="n">subNN_mkt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_net</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
</code></pre></div>

<p>So, What is the actually performance of the two models? In order to get a more robust result, we trained 1000 separate models using different structures independently, and then tested them on the test set. We took the average of the results from these 1000 tests. The result is quite surprising. In fact, the easy structure works better in our cases.</p>
<p><img alt="Neural Embeddings Approach Accuracy Results" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/Lighthouse-Securities_02_accuracy.png"></p>
<h2>4. VIX Trading Strategy</h2>
<p>Translating textual similarity into a trading signal was not an obvious next step. At first, we were skeptical whether cosine similarity—an abstract NLP metric—could carry any economically meaningful information for a volatility instrument like VIX. The initial thresholds were admittedly heuristic, chosen based on exploratory analysis rather than theory.</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">similarity</span> <span class="o">&lt;</span> <span class="mf">0.83</span><span class="p">:</span>
    <span class="n">signal</span> <span class="o">=</span> <span class="mi">1</span>   <span class="c1"># Long VIX</span>
<span class="k">elif</span> <span class="n">similarity</span> <span class="o">&gt;</span> <span class="mf">0.96</span><span class="p">:</span>
    <span class="n">signal</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Short VIX</span>
</code></pre></div>

<p>Given how easy it is to overfit a low-frequency strategy with few trades, we deliberately focused less on the maximum Sharpe ratio and more on whether performance was stable across neighboring parameters.The presence of a parameter plateau was more important to us than the single best point, as it reduced the risk that our results were driven by accidental parameter tuning.</p>
<p><img alt="Parameter Plateau Analysis" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/images/Lighthouse-Securities_02_parameter_plateau_final.png"></p>
<p>That said, the strategy trades infrequently and is heavily dependent on extreme similarity events. This makes it vulnerable to regime changes and limits its standalone applicability.</p>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2026-01-21T12:00:00+08:00">Wed 21 January 2026</time>
            <h4>Category</h4>
            <a class="category-link" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html#reflective-report-ref">Reflective Report</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html#group-lighthouse-securities-ref">Group Lighthouse Securities
                    <span class="superscript">2</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/buehlmaier/MFIN7036-student-blog-2025-12" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>