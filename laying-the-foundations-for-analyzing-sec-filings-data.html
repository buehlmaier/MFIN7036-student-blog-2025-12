<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="MFIN7036 Students 2025" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Group NLPlus+++, Literature Review, Web Scraping, Reflective Report, " />

<meta property="og:title" content="Laying the Foundations for Analyzing SEC Filings Data "/>
<meta property="og:url" content="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/laying-the-foundations-for-analyzing-sec-filings-data.html" />
<meta property="og:description" content="Blog 2 Introduction In our previous post, we discussed the nuances of preprocessing earnings calls. However, as menioned in our presentations, due to no Capital IQ API access provided by the University, along with the Bloomberg Terminal setting a daily download limit, we have pivoted our data source to focus …" />
<meta property="og:site_name" content="MFIN7036 Student Blog 2025" />
<meta property="og:article:author" content="MFIN7036 Students 2025" />
<meta property="og:article:published_time" content="2026-01-21T13:00:00+08:00" />
<meta name="twitter:title" content="Laying the Foundations for Analyzing SEC Filings Data ">
<meta name="twitter:description" content="Blog 2 Introduction In our previous post, we discussed the nuances of preprocessing earnings calls. However, as menioned in our presentations, due to no Capital IQ API access provided by the University, along with the Bloomberg Terminal setting a daily download limit, we have pivoted our data source to focus …">

        <title>Laying the Foundations for Analyzing SEC Filings Data  · MFIN7036 Student Blog 2025
</title>
        <link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="MFIN7036 Student Blog 2025 - Full Atom Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/"><span class=site-name>MFIN7036 Student Blog 2025</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://buehlmaier.github.io/MFIN7036-student-blog-2025-12
                                    >Home</a>
                                </li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html">Categories</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html">Tags</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/laying-the-foundations-for-analyzing-sec-filings-data.html">
                Laying the Foundations for Analyzing SEC Filings Data
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h2>Blog 2</h2>
<h2>Introduction</h2>
<p>In our previous post, we discussed the nuances of preprocessing earnings calls. However, as menioned in our presentations, due to no Capital IQ API access provided by the University, along with the Bloomberg Terminal setting a daily download limit, we have pivoted our data source to focus sole on 10K and 10Q filings, which are readily available to be scraped from the SEC. However, before writing a single line of code, we had to go through multiple sources of literature to gain inspiration.</p>
<p>Our project is not just about downloading data; it is about replicating and extending interesting NLP research to find predictive trading signals in financial text. In this post, we first outline the three academic pillars inspiring our strategy, and then dive into the technical engineering required to build the pipeline that supports them. It requires handling strict government rate limits, choosing the right data structures for efficiency, and selecting parsers that don't crash on messy legacy HTML.</p>
<h2>Part 1: Literature Review - Inspirations for our Research Direction</h2>
<p>Our research direction is grounded in three key studies that guide how we assess risk, value narratives, and summarize vast amounts of text.</p>
<h3>1. Beyond the 10-K: Generalizing Risk Quantification</h3>
<p>Our framework for assessing textual risk starts with <strong>Grundy and Petry’s (2020)</strong> study, <em>“10-K Risk Factors Quantification and the Information Content of Textual Reporting.”</em> They demonstrated that qualitative risk narratives in 10-K filings can be systematically quantified using machine learning (specifically LDA topic modeling). Crucially, these textual measures explain a substantial portion of cross-sectional variation in observable firm risk—proving that even “boilerplate” disclosures contain signal.</p>
<p>Grundy and Petry found that 10-K disclosures are largely contemporaneous, exhibiting limited forward-looking power. Building on this, we originally planned to generalize their framework to multiple disclosure channels, including 10-Q reports, analyst research reports, and earnings call transcripts, which differ in frequency and incentives. However, we have settled on 10K/10Q only due to the aforementioned issues regarding data constrnaints. Though one of the paper's conclusions was that 10K text contains little forward-looking predictive power, we aim to assess this ourselves, and merge them with other NLP preprocessing/feature engineering approaches to examine whether combined textual indicators can unlock forward-looking predictive capabilities in both predicting returns and realized volatility.</p>
<h3>2. Mining Value in Analyst Reports</h3>
<p>For our trading strategy, we draw inspiration from the study <em>“Do Sell-side Analyst Reports Have Investment Value?”</em> (2025). This research utilizes Large Language Models (LLMs) and Shapley value decomposition to extract investment value from qualitative narratives.</p>
<p>The study found that a long-short strategy based on text-derived predictions generated significant alpha (1.04% monthly). Interestingly, this predictability often stems from reports featuring short-term negative sentiment but long-term optimistic fundamentals, which is a pattern reflecting market overreaction to near-term bad news. Furthermore, the “Strategic Outlook” sections of these reports were found to drive a massive 41% of the portfolio's Sharpe ratio. If we could replicate this “Strategic Outlook” filtering to isolate high-value signals from noise, then we could use LLM-derived embeddings to capture the nuance between temporary setbacks and fundamental decline. However, as we found difficulties with obtaining a rich dataset of many sell-side analyst reports, we focused on the 10-K paper instead.</p>
<h3>3. Taming Long Documents with ECTSum</h3>
<p>Finally, handling the sheer volume of texts may require advanced summarization techniques. We looked to adopt the framework from Mukherjee et al.’s <em>“ECTSum: A New Benchmark Dataset for Bullet Point Summarization of Long Earnings Call Transcripts.”</em></p>
<p>ECTSum, the first large-scale financial long-document summarization dataset, comprises 2,425 pairs of unstructured earnings call transcripts (average 2.9K words) and expert-written telegram-style bullet-point summaries, with an ultra-high compression ratio of 103.67. The ECT-BPS framework they developed integrates a FinBERT-enhanced extractive module with a T5-based paraphrasing module. It has been shown to outperform state-of-the-art models in ROUGE, BERTScore, factual consistency metrics, and financial expert evaluations. Notably, both the dataset and ECT-BPS are publicly available on GitHub. Building on earnings call transcripts, we aim to explore ECT-BPS’s application potential in 10-K and 10-Q filings, to investigate whether key financial information extracted via ECTSum can unlock novel predictive capabilities for financial analysis. Should using the ECTSum methodology or training our own model prove too computationally expensive, running pre-trained or open-source summarizers such as BART are also viable.</p>
<h2>Part 2: Engineering the Pipeline</h2>
<p>With our research directions mostly set, the next challenge was execution. Building a system to analyze many years of SEC filings isn't just about writing a script that works once only, but about building a reproducible, robust data engine with structured outputs.</p>
<p>We redesigned our workflow into a three-stage pipeline: <strong>Universe Construction → High-Concurrency Scraping → Surgical Extraction.</strong></p>
<h3>1. Defining the Universe: Quality Over Quantity</h3>
<p><strong>Corresponding Workflow:</strong> <code>wrds_data</code> → <code>build_cik_universe</code></p>
<p>Before downloading a single PDF, we had to answer: Which companies matter?</p>
<p>A naive approach would be to download filings for every company listed on the SEC. However, this introduces noise (shell companies, penny stocks) and ignores economic reality. Furthermore, simply using today's S&amp;P 500 constituents to backtest data from 2010 introduces Survivorship Bias, i.e. we would miss companies like Kodak or Lehman Brothers that were significant then but are gone now.</p>
<p><strong>Our Solution: Dynamic Annual Rebalancing</strong></p>
<p>We built a “Universe Construction” module that ingests raw market data (from WRDS/CRSP).</p>
<ul>
<li><strong>Market Cap Ranking:</strong> For every year (e.g., June 2010, June 2011...), we rank all public firms by market capitalization</li>
<li><strong>Top N Selection:</strong> We dynamically select the Top 100 firms for each year</li>
<li><strong>Identifier Mapping:</strong> We map financial identifiers (GVKEY/PERMNO) to SEC identifiers (CIK) to ensure our financial data aligns perfectly with the text data</li>
</ul>
<p>This ensures our model trains on the most economically significant companies at that point in time.</p>
<h3>2. The Scraper: Rate Limiting &amp; Concurrency</h3>
<p><strong>Corresponding Workflow:</strong> <code>sec_scrape_filings</code></p>
<p>The SEC's EDGAR database is a goldmine of textual data, but online sources cited that it enforces a rate limit, resulting in a risk of an IP ban if it is exceeded. A simple single-threaded loop is too slow for thousands of files, but uncontrolled multi-threading triggers bans.</p>
<p><strong>Our Solution: The Token Bucket Algorithm</strong></p>
<p>Instead of simple <code>time.sleep()</code>, we implemented a Token Bucket rate limiter.</p>
<ul>
<li><strong>Concept:</strong> Imagine a bucket that fills with “tokens” at a rate of 10 per second. Every download thread must “pay” a token to send a request. If the bucket is empty, the thread waits.</li>
<li><strong>Concurrency:</strong> This allowed us to wrap our scraper in a <code>ThreadPoolExecutor</code>, launching multiple download workers simultaneously.</li>
<li><strong>Safety:</strong> The global Token Bucket ensures that even with 10 parallel threads, the aggregate traffic never violates SEC rules.</li>
</ul>
<p>This architecture maximizes throughput while maintaining 100% compliance with government API limits.</p>
<h3>3. Surgical Extraction: Parsing “Tag Soup” with lxml</h3>
<p>Financial filings are notoriously messy. While modern HTML is structured, older 10-K filings often contain broken tags, unclosed elements, and inconsistent formatting (often called “tag soup”). A risk model fed with the “Table of Contents” or “Page Headers” will fail.</p>
<p><strong>Targeted Regex Extraction</strong></p>
<p>We developed a library of Regular Expressions to identify the specific boundaries of key sections in both 10-K (Annual) and 10-Q (Quarterly) reports:</p>
<ul>
<li><strong>Item 1A (Risk Factors):</strong> Captures the company's self-assessed risks.</li>
<li><strong>Item 7 (MD&amp;A):</strong> Captures management's narrative on performance.</li>
<li><strong>Item 3 (Legal Proceedings):</strong> Captures litigation risks.</li>
</ul>
<p>To extract text from these files, we used the BeautifulSoup library. However, BeautifulSoup is just a wrapper; it requires an underlying parser to do the heavy lifting. We specifically chose <code>lxml</code> over Python's built-in <code>html.parser</code>.</p>
<p><strong>Why <code>lxml</code>?</strong></p>
<ol>
<li><strong>Speed:</strong> <code>lxml</code> is written in C, making it significantly faster at parsing large HTML trees than Python's built-in tools.</li>
<li><strong>Lenience &amp; Stability:</strong> Financial data is often messy. <code>lxml</code> is extremely robust at handling broken HTML without crashing or throwing errors. It attempts to “fix” the structure as it reads, ensuring we extract usable text even from poorly formatted filings.</li>
</ol>
<p><strong>Implementation:</strong></p>
<p>In our <code>extract_text_from_html</code> function, you will see this specific argument:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="k">def</span> <span class="nf">extract_text_from_html</span><span class="p">(</span><span class="n">html_content</span><span class="p">):</span>
    <span class="c1"># &#39;lxml&#39; is explicitly defined as the parser</span>
    <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">html_content</span><span class="p">,</span> <span class="s1">&#39;lxml&#39;</span><span class="p">)</span>

    <span class="c1"># Clean up scripts and styles</span>
    <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">soup</span><span class="p">([</span><span class="s1">&#39;script&#39;</span><span class="p">,</span> <span class="s1">&#39;style&#39;</span><span class="p">]):</span>
        <span class="n">element</span><span class="o">.</span><span class="n">decompose</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">soup</span><span class="o">.</span><span class="n">get_text</span><span class="p">(</span><span class="n">separator</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</code></pre></div>

<h3>4. Micro-Optimization: Why Set Beats List</h3>
<p>When processing thousands of financial documents, “small” inefficiencies compound into hours of wasted compute time. One optimization we implemented in our preprocessing code (shared previously) is using Sets (<code>set</code>) instead of Lists (<code>list</code>) for tasks like stopword removal.</p>
<p>**Why: O(.) complexity</p>
<ul>
<li><strong>List Lookup:</strong> <code>O(n)</code> complexity. Checking if a word exists in a list (e.g., <code>if word in stop_words_list</code>) requires scanning the whole list until a match is found.</li>
<li><strong>Set Lookup:</strong> <code>O(1)</code> complexity. Hash tables make lookups instant on average.</li>
</ul>
<p><strong>The Impact:</strong></p>
<p>A standard stopword list might contain ~180 words. A 10-K filing can easily contain 50,000+ words.</p>
<ol>
<li><strong>Using a List:</strong> 50,000 words × 180 checks = ~9,000,000 operations per document.</li>
<li><strong>Using a Set:</strong> 50,000 words × 1 check = ~50,000 operations per document.</li>
</ol>
<p>In our code, this looks like:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Inefficient</span>
<span class="n">stop_words_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;at&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

<span class="c1"># Efficient</span>
<span class="n">STOP_WORDS</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>

<span class="c1"># usage</span>
<span class="n">processed_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">STOP_WORDS</span><span class="p">]</span>
</code></pre></div>

<p>This simple change drastically reduced our preprocessing time, allowing us to iterate on our LDA models much faster.</p>
<h2>Conclusion</h2>
<p>Data engineering often goes unnoticed in the final results and findings of trading strategies and predictions, but it is the backbone of our project's evaluation of code quality. By iterating and moving from a simple script to a  pipeline that declares a stock universe, limites scraping rates, and downloads specific 10-K/10-Q sections in a robust manner, we have built a resilient foundation capable of handling the scale required for our research.</p>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2026-01-21T13:00:00+08:00">Wed 21 January 2026</time>
            <h4>Category</h4>
            <a class="category-link" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/categories.html#reflective-report-ref">Reflective Report</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html#group-nlplus-ref">Group NLPlus+++
                    <span class="superscript">2</span>
</a></li>
                <li><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html#literature-review-ref">Literature Review
                    <span class="superscript">1</span>
</a></li>
                <li><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/tags.html#web-scraping-ref">Web Scraping
                    <span class="superscript">1</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/buehlmaier/MFIN7036-student-blog-2025-12" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-12/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>